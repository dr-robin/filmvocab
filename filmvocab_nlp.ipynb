{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP to retrieve parts of speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "pd.set_option('display.max_colwidth', 170) #widen pandas rows display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "#file = 'dataset/example_train_imdb_reviews.csv'\n",
    "#df = pd.read_csv(file)\n",
    "#df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a film subtitle file\n",
    "file = 'top gun-English.sub'\n",
    "with open(file) as f:\n",
    "    l = f.read().splitlines()\n",
    "\n",
    "#Strip unwanted characters\n",
    "import re\n",
    "li = [re.sub(r'({.*})|(\\|-)|(\\|)|', '', i) for i in l]\n",
    "s = str(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load eng model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#Tokenise\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lists of token types of interest\n",
    "tokens_pos = [token.pos_ for token in doc]\n",
    "tokens_text = [token.text for token in doc]\n",
    "tokens_lemma = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert token lists to dataframe\n",
    "d = zip(tokens_pos, tokens_text, tokens_lemma)\n",
    "df = pd.DataFrame(d)\n",
    "df.columns = ('partofspeech', 'text', 'lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = df[df.partofspeech == 'PUNCT'].groupby('partofspeech')['text'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop PUNCT and SYM\n",
    "df = df[df.partofspeech != 'PUNCT']\n",
    "df = df[df.partofspeech != 'SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6476, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partofspeech\n",
       "ADJ      178\n",
       "ADP       41\n",
       "ADV      124\n",
       "AUX       26\n",
       "CCONJ      8\n",
       "DET       44\n",
       "INTJ      27\n",
       "NOUN     503\n",
       "NUM       51\n",
       "PART       8\n",
       "PRON      38\n",
       "PROPN    159\n",
       "SCONJ     18\n",
       "VERB     405\n",
       "X          1\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get counts of pos\n",
    "pos_count = df.groupby('partofspeech')['text'].nunique()\n",
    "pos_count\n",
    "#Access to pos counts\n",
    "pos_count['ADJ']\n",
    "pos_count['ADP']\n",
    "pos_count['ADV']\n",
    "pos_count['AUX']\n",
    "pos_count['CCONJ']\n",
    "pos_count['DET']\n",
    "pos_count['INTJ']\n",
    "pos_count['NOUN']\n",
    "pos_count['NUM']\n",
    "pos_count['PART']\n",
    "pos_count['PRON']\n",
    "pos_count['PROPN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partofspeech</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUX</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CCONJ</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INTJ</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PART</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PROPN</th>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCONJ</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text\n",
       "partofspeech      \n",
       "ADJ            178\n",
       "ADP             41\n",
       "ADV            124\n",
       "AUX             26\n",
       "CCONJ            8\n",
       "DET             44\n",
       "INTJ            27\n",
       "NOUN           503\n",
       "NUM             51\n",
       "PART             8\n",
       "PRON            38\n",
       "PROPN          159\n",
       "SCONJ           18\n",
       "VERB           405\n",
       "X                1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = df.pivot_table('text', 'partofspeech', aggfunc=pd.Series.nunique)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1631"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_count = pos_count.sum()\n",
    "vocab_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partofspeech\n",
       "ADJ       346\n",
       "ADP       476\n",
       "ADV       407\n",
       "AUX       658\n",
       "CCONJ      85\n",
       "DET       640\n",
       "INTJ       86\n",
       "NOUN      921\n",
       "NUM       136\n",
       "PART      216\n",
       "PRON     1033\n",
       "PROPN     398\n",
       "SCONJ      69\n",
       "VERB     1004\n",
       "X           1\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_size = df.groupby('partofspeech')['text'].size()\n",
    "print(pos_size)\n",
    "\n",
    "#Access to parts of speech\n",
    "pos_size['ADJ']\n",
    "pos_size['ADP'] #etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text\n",
       "'              2\n",
       "'d             9\n",
       "'ll           25\n",
       "'m            54\n",
       "'re           59\n",
       "              ..\n",
       "yours          2\n",
       "yourself       2\n",
       "youwere        1\n",
       "youwitness     1\n",
       "zone           1\n",
       "Length: 1484, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = df.groupby('text').size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "piv = pd.pivot_table(df, index = ['partofspeech','text'], values = 'text', aggfunc=np.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partofspeech</th>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ADJ</th>\n",
       "      <th>-</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12of</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Better</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dangerous</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dead</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">VERB</th>\n",
       "      <th>worry</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wounded</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <th>I'm</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1631 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        lemma\n",
       "partofspeech text            \n",
       "ADJ          -              1\n",
       "             12of           1\n",
       "             Better         1\n",
       "             Dangerous      1\n",
       "             Dead           1\n",
       "...                       ...\n",
       "VERB         worry          2\n",
       "             would          6\n",
       "             wounded        1\n",
       "             writing        1\n",
       "X            I'm            1\n",
       "\n",
       "[1631 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " \"'\",\n",
       " 'Morning',\n",
       " ',',\n",
       " 'Scott',\n",
       " '.',\n",
       " 'Morning',\n",
       " ',',\n",
       " 'Wells',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Ghost',\n",
       " 'Rider',\n",
       " ',',\n",
       " 'we',\n",
       " 'have',\n",
       " 'an',\n",
       " 'unknownaircraft',\n",
       " '.',\n",
       " 'Vector',\n",
       " '090',\n",
       " 'for',\n",
       " 'bogey',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Who',\n",
       " \"'s\",\n",
       " 'up',\n",
       " 'there',\n",
       " '?',\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'Merlin',\n",
       " ',',\n",
       " 'Maverick',\n",
       " 'and',\n",
       " 'Goose',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Great',\n",
       " '.',\n",
       " 'Maverick',\n",
       " 'and',\n",
       " 'Goose',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " 'Talk',\n",
       " 'to',\n",
       " 'me',\n",
       " ',',\n",
       " 'Goose',\n",
       " '.',\n",
       " 'Roger',\n",
       " ',',\n",
       " 'I',\n",
       " 'got',\n",
       " 'them',\n",
       " '.',\n",
       " '900',\n",
       " 'knots',\n",
       " 'closure',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'that',\n",
       " '?',\n",
       " 'Roger',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Merlin',\n",
       " ',',\n",
       " 'you',\n",
       " 'got',\n",
       " 'them',\n",
       " '?',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'radar',\n",
       " 'contact',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'get',\n",
       " 'a',\n",
       " 'visual',\n",
       " 'ID',\n",
       " '.',\n",
       " 'You',\n",
       " 'hook',\n",
       " 'them',\n",
       " '.',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'clean',\n",
       " 'them',\n",
       " 'and',\n",
       " 'fry',\n",
       " 'them',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Ghost',\n",
       " 'Rider',\n",
       " ',',\n",
       " '203',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'them',\n",
       " 'inbound',\n",
       " ',',\n",
       " 'bogeyheading',\n",
       " '270',\n",
       " 'at',\n",
       " '10',\n",
       " 'miles',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " '900',\n",
       " 'knots',\n",
       " 'closure',\n",
       " '.',\n",
       " 'Take',\n",
       " 'angels',\n",
       " '10',\n",
       " ',',\n",
       " 'left',\n",
       " ',',\n",
       " '3',\n",
       " ',',\n",
       " '0',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " 'Are',\n",
       " 'we',\n",
       " 'expecting',\n",
       " 'visitors',\n",
       " '?',\n",
       " 'Negative',\n",
       " ',',\n",
       " 'sir',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Merlin',\n",
       " ',',\n",
       " 'see',\n",
       " 'if',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'single',\n",
       " '.',\n",
       " 'Roger',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'a',\n",
       " 'trailer?.Looks',\n",
       " 'like',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'single',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'gon',\n",
       " 'na',\n",
       " 'go',\n",
       " 'head',\n",
       " '-',\n",
       " 'to',\n",
       " '-',\n",
       " 'head',\n",
       " 'with',\n",
       " 'him',\n",
       " '.',\n",
       " 'Take',\n",
       " 'it',\n",
       " 'easy',\n",
       " '.',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'if',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'really',\n",
       " 'alone',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'He',\n",
       " \"'s\",\n",
       " 'coming',\n",
       " 'right',\n",
       " 'at',\n",
       " 'us',\n",
       " ',',\n",
       " 'Mav',\n",
       " '.',\n",
       " 'Okay',\n",
       " ',',\n",
       " 'buddy',\n",
       " '.',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'on',\n",
       " 'your',\n",
       " 'mind',\n",
       " '?',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Shit',\n",
       " ',',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'MiG-28s',\n",
       " '!',\n",
       " 'What',\n",
       " '?',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'their',\n",
       " 'position',\n",
       " '?',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " '250',\n",
       " 'miles',\n",
       " 'out',\n",
       " ',',\n",
       " 'sir',\n",
       " '.',\n",
       " 'Get',\n",
       " 'them',\n",
       " 'out',\n",
       " 'of',\n",
       " 'here',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'MiG',\n",
       " 'one',\n",
       " '.',\n",
       " \"I'm\",\n",
       " 'going',\n",
       " 'after',\n",
       " 'MiG',\n",
       " 'two',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'I',\n",
       " 'lost',\n",
       " 'him',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sun',\n",
       " '!',\n",
       " 'Shit',\n",
       " ',',\n",
       " \"what'she\",\n",
       " 'doing',\n",
       " '?',\n",
       " 'You',\n",
       " 'got',\n",
       " 'him',\n",
       " ',',\n",
       " 'Merlin',\n",
       " '?',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'He',\n",
       " \"'s\",\n",
       " 'coming',\n",
       " 'around',\n",
       " 'on',\n",
       " 'our',\n",
       " 'tail',\n",
       " '!',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Goddamn',\n",
       " 'it',\n",
       " '.',\n",
       " 'This',\n",
       " 'bogey',\n",
       " \"'s\",\n",
       " 'all',\n",
       " 'over',\n",
       " 'me',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'their',\n",
       " 'range',\n",
       " 'now',\n",
       " '?',\n",
       " '.',\n",
       " 'If',\n",
       " 'they',\n",
       " 'break150',\n",
       " 'miles',\n",
       " ',',\n",
       " 'launch',\n",
       " 'the',\n",
       " 'alert',\n",
       " 'five',\n",
       " 'aircraft',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Airspeed',\n",
       " '300',\n",
       " '.',\n",
       " 'Go',\n",
       " 'get',\n",
       " 'them',\n",
       " ',',\n",
       " 'Mav',\n",
       " '!',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'for',\n",
       " 'missile',\n",
       " 'lock',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'see',\n",
       " 'if',\n",
       " 'we',\n",
       " 'can',\n",
       " 'scare',\n",
       " 'himout',\n",
       " 'of',\n",
       " 'here',\n",
       " '.',\n",
       " 'Lock',\n",
       " 'up',\n",
       " ',',\n",
       " 'baby',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'I',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'him',\n",
       " 'locked',\n",
       " '.',\n",
       " 'Bingo',\n",
       " '!',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'We',\n",
       " 'got',\n",
       " 'him',\n",
       " ',',\n",
       " 'Mav',\n",
       " '.',\n",
       " 'He',\n",
       " \"'s\",\n",
       " 'bugging',\n",
       " 'out',\n",
       " '.',\n",
       " 'Mustang',\n",
       " ',',\n",
       " 'MiG',\n",
       " 'two',\n",
       " 'is',\n",
       " 'headed',\n",
       " 'home',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'What',\n",
       " \"'s\",\n",
       " 'their',\n",
       " 'position',\n",
       " '?',\n",
       " '180',\n",
       " 'miles',\n",
       " 'and',\n",
       " 'bearing',\n",
       " '010',\n",
       " ',',\n",
       " 'sir',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'He',\n",
       " \"'s\",\n",
       " 'got',\n",
       " 'missile',\n",
       " 'lock',\n",
       " 'on',\n",
       " 'us',\n",
       " '!',\n",
       " 'He',\n",
       " \"'s\",\n",
       " 'engaging',\n",
       " 'me',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Goddamn',\n",
       " 'it',\n",
       " '.',\n",
       " 'Mustang',\n",
       " ',',\n",
       " 'this',\n",
       " 'bogey',\n",
       " \"'s\",\n",
       " 'all',\n",
       " 'over',\n",
       " 'me',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " 'Do',\n",
       " 'I',\n",
       " 'have',\n",
       " 'permission',\n",
       " 'to',\n",
       " 'fire',\n",
       " '?',\n",
       " 'Do',\n",
       " 'not',\n",
       " 'fire',\n",
       " 'until',\n",
       " 'fired',\n",
       " 'upon',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'There',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'The',\n",
       " 'MiG',\n",
       " \"'s\",\n",
       " 'in',\n",
       " 'perfect',\n",
       " 'firing',\n",
       " 'position',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " 'No',\n",
       " 'way',\n",
       " '.',\n",
       " 'He',\n",
       " 'would',\n",
       " 'have',\n",
       " 'fired',\n",
       " 'by',\n",
       " 'now',\n",
       " '.',\n",
       " 'Maverick',\n",
       " ',',\n",
       " 'get',\n",
       " 'this',\n",
       " 'asshole',\n",
       " 'off',\n",
       " 'me',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Easy',\n",
       " ',',\n",
       " 'Cougar',\n",
       " '.',\n",
       " 'Bring',\n",
       " 'him',\n",
       " 'back',\n",
       " ',',\n",
       " 'hard',\n",
       " 'right',\n",
       " '.',\n",
       " 'Help',\n",
       " 'me',\n",
       " 'engage',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'on',\n",
       " 'my',\n",
       " 'way',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'I',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'shoot',\n",
       " ',',\n",
       " 'so',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'have',\n",
       " 'a',\n",
       " 'little',\n",
       " 'fun',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Is',\n",
       " 'this',\n",
       " 'your',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'fun',\n",
       " '?',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Greetings',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Watch',\n",
       " 'the',\n",
       " 'birdie',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Geez',\n",
       " ',',\n",
       " 'I',\n",
       " 'crack',\n",
       " 'myself',\n",
       " 'up',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Great',\n",
       " 'shot',\n",
       " '.',\n",
       " 'I',\n",
       " 'should',\n",
       " 'be',\n",
       " 'a',\n",
       " 'photographer',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'your',\n",
       " 'tail',\n",
       " 'is',\n",
       " 'clear',\n",
       " '.',\n",
       " 'MiG',\n",
       " 'one',\n",
       " 'has',\n",
       " 'bugged',\n",
       " 'out',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'had',\n",
       " 'enough',\n",
       " 'funfor',\n",
       " 'one',\n",
       " 'day',\n",
       " ',',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'you',\n",
       " 'think',\n",
       " '?',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'getting',\n",
       " 'low',\n",
       " 'on',\n",
       " 'fuel',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'heading',\n",
       " 'home',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'We',\n",
       " \"'ll\",\n",
       " 'see',\n",
       " 'you',\n",
       " 'on',\n",
       " 'deck',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'running',\n",
       " 'low',\n",
       " 'on',\n",
       " 'gas',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Cougar',\n",
       " '?',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'on',\n",
       " 'vapour',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'do',\n",
       " 'it',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'come',\n",
       " 'on',\n",
       " '!',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'take',\n",
       " 'ltback',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ship',\n",
       " ',',\n",
       " 'man',\n",
       " '.',\n",
       " 'Get',\n",
       " 'him',\n",
       " 'on',\n",
       " 'deck',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'possible',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'Cougar',\n",
       " ',',\n",
       " 'we',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'to',\n",
       " 'land',\n",
       " 'this',\n",
       " 'thing',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'way',\n",
       " 'low',\n",
       " 'on',\n",
       " 'gas',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Are',\n",
       " 'you',\n",
       " 'all',\n",
       " 'right',\n",
       " '?',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '-',\n",
       " 'Maverick',\n",
       " ',',\n",
       " 'call',\n",
       " 'the',\n",
       " 'ball',\n",
       " '.',\n",
       " 'Roger',\n",
       " '.',\n",
       " 'Maverick',\n",
       " 'has',\n",
       " 'the',\n",
       " 'ball',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Cougar',\n",
       " \"'s\",\n",
       " 'in',\n",
       " 'trouble',\n",
       " '.',\n",
       " 'Come',\n",
       " 'on',\n",
       " ',',\n",
       " 'we',\n",
       " \"'re\",\n",
       " 'getting',\n",
       " 'low',\n",
       " 'on',\n",
       " 'gas',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'land',\n",
       " 'this',\n",
       " 'sucker',\n",
       " '.',\n",
       " 'Cougar',\n",
       " '!',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Goddamn',\n",
       " 'it',\n",
       " '!',\n",
       " 'We',\n",
       " \"'ve\",\n",
       " 'got',\n",
       " 'no',\n",
       " 'fuel',\n",
       " 'for',\n",
       " 'this',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'We',\n",
       " \"'re\",\n",
       " 'going',\n",
       " 'after',\n",
       " 'Cougar',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " '-',\n",
       " 'Tell',\n",
       " 'him',\n",
       " 'to',\n",
       " 'land',\n",
       " '.',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'an',\n",
       " 'order',\n",
       " '!',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'instructed',\n",
       " 'to',\n",
       " 'land',\n",
       " 'immediately',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Have',\n",
       " 'you',\n",
       " 'boys',\n",
       " 'seen',\n",
       " 'an',\n",
       " 'aircraft',\n",
       " 'carrier',\n",
       " '?',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'That',\n",
       " 'MiG',\n",
       " 'really',\n",
       " 'screwed',\n",
       " 'him',\n",
       " 'up',\n",
       " '.',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'think',\n",
       " 'he',\n",
       " 'can',\n",
       " 'make',\n",
       " 'it',\n",
       " 'back',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " '\"',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'okay',\n",
       " ',',\n",
       " 'Cougar',\n",
       " '.',\n",
       " 'Just',\n",
       " 'stay',\n",
       " 'onmy',\n",
       " 'wing',\n",
       " '.',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'take',\n",
       " 'you',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'in',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Easy',\n",
       " ',',\n",
       " 'Cougar',\n",
       " '.',\n",
       " 'Just',\n",
       " 'a',\n",
       " 'walk',\n",
       " 'in',\n",
       " 'the',\n",
       " 'park',\n",
       " ',',\n",
       " 'buddy',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'a',\n",
       " 'little',\n",
       " 'low',\n",
       " '.',\n",
       " 'You',\n",
       " \"'re\",\n",
       " 'a',\n",
       " 'little',\n",
       " 'low',\n",
       " '.',\n",
       " 'Come',\n",
       " 'on',\n",
       " ',',\n",
       " 'buddy',\n",
       " ',',\n",
       " 'pull',\n",
       " 'up',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'Pull',\n",
       " 'up',\n",
       " ',',\n",
       " 'Cougar',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '\"',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'it',\n",
       " '.',\n",
       " 'Almost',\n",
       " 'there',\n",
       " '.',\n",
       " '\"',\n",
       " ',',\n",
       " \"'\",\n",
       " 'A',\n",
       " 'little',\n",
       " 'below',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "len(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1\n",
      "' 726\n",
      "Morning 2\n",
      ", 1065\n",
      "Scott 1\n",
      ". 986\n",
      "Wells 1\n",
      "Ghost 3\n",
      "Rider 3\n",
      "we 20\n",
      "have 42\n",
      "an 10\n",
      "unknownaircraft 1\n",
      "Vector 1\n",
      "090 4\n",
      "for 34\n",
      "bogey 9\n",
      "\" 717\n",
      "- 233\n",
      "Who 3\n",
      "'s 170\n",
      "up 36\n",
      "there 33\n",
      "? 131\n",
      "Cougar 22\n",
      "Merlin 4\n",
      "Maverick 36\n",
      "and 39\n",
      "Goose 19\n",
      "Great 8\n",
      "Talk 3\n",
      "to 110\n",
      "me 45\n",
      "Roger 8\n",
      "I 251\n",
      "got 63\n",
      "them 18\n",
      "900 2\n",
      "knots 7\n",
      "closure 2\n",
      "you 166\n",
      "hear 3\n",
      "that 53\n",
      "'ve 45\n",
      "radar 5\n",
      "contact 1\n",
      "'ll 25\n",
      "get 22\n",
      "a 125\n",
      "visual 1\n",
      "ID 1\n",
      "You 98\n",
      "hook 1\n",
      "clean 2\n",
      "fry 1\n",
      "203 1\n",
      "inbound 1\n",
      "bogeyheading 1\n",
      "270 1\n",
      "at 14\n",
      "10 10\n",
      "miles 16\n",
      "Take 8\n",
      "angels 1\n",
      "left 11\n",
      "3 2\n",
      "0 1\n",
      "Are 4\n",
      "expecting 1\n",
      "visitors 1\n",
      "Negative 3\n",
      "sir 25\n",
      "see 20\n",
      "if 6\n",
      "he 31\n",
      "single 2\n",
      "Do 10\n",
      "trailer?.Looks 1\n",
      "like 17\n",
      "'m 54\n",
      "gon 16\n",
      "na 16\n",
      "go 20\n",
      "head 4\n",
      "with 31\n",
      "him 47\n",
      "it 94\n",
      "easy 1\n",
      "really 8\n",
      "alone 4\n",
      "He 47\n",
      "coming 11\n",
      "right 37\n",
      "us 14\n",
      "Mav 13\n",
      "Okay 4\n",
      "buddy 3\n",
      "What 23\n",
      "on 90\n",
      "your 52\n",
      "mind 7\n",
      "Shit 8\n",
      "two 15\n",
      "of 61\n",
      "MiG-28s 1\n",
      "! 81\n",
      "their 5\n",
      "position 6\n",
      "250 1\n",
      "out 27\n",
      "Get 14\n",
      "here 25\n",
      "MiG 26\n",
      "one 26\n",
      "I'm 6\n",
      "going 19\n",
      "after 7\n",
      "lost 11\n",
      "in 67\n",
      "the 155\n",
      "sun 1\n",
      "what'she 1\n",
      "doing 10\n",
      "around 4\n",
      "our 11\n",
      "tail 14\n",
      "Goddamn 7\n",
      "This 22\n",
      "all 17\n",
      "over 13\n",
      "range 2\n",
      "now 9\n",
      "If 9\n",
      "they 7\n",
      "break150 1\n",
      "launch 2\n",
      "alert 2\n",
      "five 9\n",
      "aircraft 6\n",
      "Airspeed 1\n",
      "300 2\n",
      "Go 3\n",
      "missile 6\n",
      "lock 5\n",
      "Let 15\n",
      "can 16\n",
      "scare 1\n",
      "himout 1\n",
      "Lock 1\n",
      "baby 3\n",
      "locked 2\n",
      "Bingo 3\n",
      "We 43\n",
      "bugging 2\n",
      "Mustang 2\n",
      "is 51\n",
      "headed 1\n",
      "home 6\n",
      "180 2\n",
      "bearing 2\n",
      "010 1\n",
      "engaging 2\n",
      "this 26\n",
      "permission 1\n",
      "fire 8\n",
      "not 20\n",
      "until 1\n",
      "fired 2\n",
      "upon 1\n",
      "There 13\n",
      "are 31\n",
      "The 32\n",
      "perfect 2\n",
      "firing 4\n",
      "No 17\n",
      "way 9\n",
      "would 6\n",
      "by 9\n",
      "asshole 2\n",
      "off 10\n",
      "Easy 2\n",
      "Bring 1\n",
      "back 11\n",
      "hard 14\n",
      "Help 1\n",
      "engage 3\n",
      "my 25\n",
      "ca 19\n",
      "n't 78\n",
      "shoot 1\n",
      "so 9\n",
      "let 7\n",
      "little 6\n",
      "fun 2\n",
      "Is 6\n",
      "idea 2\n",
      "Greetings 1\n",
      "Watch 6\n",
      "birdie 1\n",
      "Geez 1\n",
      "crack 1\n",
      "myself 1\n",
      "shot 16\n",
      "should 5\n",
      "be 35\n",
      "photographer 1\n",
      "clear 3\n",
      "has 11\n",
      "bugged 1\n",
      "had 8\n",
      "enough 5\n",
      "funfor 1\n",
      "day 3\n",
      "do 41\n",
      "think 11\n",
      "getting 4\n",
      "low 9\n",
      "fuel 2\n",
      "'re 59\n",
      "heading 2\n",
      "deck 7\n",
      "running 1\n",
      "gas 3\n",
      "vapour 1\n",
      "come 10\n",
      "take 10\n",
      "ltback 1\n",
      "ship 3\n",
      "man 13\n",
      "as 6\n",
      "soon 3\n",
      "possible 1\n",
      "land 4\n",
      "thing 7\n",
      "call 7\n",
      "ball 3\n",
      "trouble 2\n",
      "Come 20\n",
      "sucker 1\n",
      "no 10\n",
      "Tell 3\n",
      "That 27\n",
      "order 1\n",
      "instructed 1\n",
      "immediately 1\n",
      "Have 1\n",
      "boys 2\n",
      "seen 5\n",
      "carrier 1\n",
      "screwed 2\n",
      "make 5\n",
      "okay 5\n",
      "Just 8\n",
      "stay 2\n",
      "onmy 1\n",
      "wing 1\n",
      "walk 2\n",
      "park 2\n",
      "pull 1\n",
      "Pull 1\n",
      "Almost 1\n",
      "A 8\n",
      "below 5\n",
      "glide 1\n",
      "path 1\n",
      "Call 2\n",
      "Rogerball 1\n",
      "too 10\n",
      "Increase 1\n",
      "power 1\n",
      "My 10\n",
      "wife 1\n",
      "kid 8\n",
      "almost 1\n",
      "orphaned 1\n",
      "today 1\n",
      "I've 2\n",
      "never 6\n",
      "even 3\n",
      "was 41\n",
      "scared 1\n",
      "before 4\n",
      "holding 1\n",
      "tight 2\n",
      "edge 1\n",
      "sorry 6\n",
      "Thanks 4\n",
      "hell 10\n",
      "know 29\n",
      "just 20\n",
      "did 21\n",
      "incredibly 1\n",
      "brave 1\n",
      "done 3\n",
      "landyour 1\n",
      "plane 4\n",
      "It 24\n",
      "belongs 1\n",
      "taxpayers 1\n",
      "Your 6\n",
      "ego 1\n",
      "writing 1\n",
      "chequesyour 1\n",
      "body 1\n",
      "cash 1\n",
      "qualificationsas 1\n",
      "section 1\n",
      "leader 1\n",
      "three 5\n",
      "times 1\n",
      "Put 1\n",
      "hack 1\n",
      "twice 3\n",
      "mewith 1\n",
      "history 2\n",
      "high 2\n",
      "speed 3\n",
      "passes 1\n",
      "air 3\n",
      "control 5\n",
      "towersand 1\n",
      "admiral 1\n",
      "daughter 2\n",
      "Penny 2\n",
      "Benjamin 2\n",
      "lucky 2\n",
      "bullshit 1\n",
      "family 4\n",
      "nameain't 1\n",
      "best 18\n",
      "Navy 2\n",
      "need 7\n",
      "itbetter 1\n",
      "cleaner 1\n",
      "than 9\n",
      "other 3\n",
      "guy 8\n",
      "want 10\n",
      "serve 1\n",
      "country 1\n",
      "screw 2\n",
      "pilot 10\n",
      "Maybe 3\n",
      "good 19\n",
      "I'd 1\n",
      "bust 1\n",
      "butt 2\n",
      "but 15\n",
      "another 5\n",
      "problem 7\n",
      "send 2\n",
      "somebody 2\n",
      "Miramar 1\n",
      "something 7\n",
      "believeit 1\n",
      "give 4\n",
      "dream 1\n",
      "against 5\n",
      "characters 1\n",
      "Top 10\n",
      "Gun 9\n",
      "For 3\n",
      "weeks 2\n",
      "you're 1\n",
      "fly 14\n",
      "were 17\n",
      "number 3\n",
      "wasnumber 1\n",
      "turned 1\n",
      "his 9\n",
      "wings 1\n",
      "guys 3\n",
      "But 15\n",
      "remember 1\n",
      ": 1\n",
      "flyinga 1\n",
      "cargo 1\n",
      "full 3\n",
      "rubber 1\n",
      "dog 2\n",
      "shit 9\n",
      "Yes 9\n",
      "about 21\n",
      "some 9\n",
      "time 11\n",
      "Gentlemen 5\n",
      "Good 4\n",
      "luck 2\n",
      "gentlemen 2\n",
      "Thank 6\n",
      "During 2\n",
      "Korea 1\n",
      "down 13\n",
      "12of 1\n",
      "theirjets 1\n",
      "every 2\n",
      "ours 1\n",
      "Vietnam 3\n",
      "ratio 2\n",
      "fell 2\n",
      "1 4\n",
      ".Our 1\n",
      "pilots 3\n",
      "dependant 1\n",
      "missiles 2\n",
      "created 1\n",
      "teach 2\n",
      "ACM.Air 1\n",
      "Combat 1\n",
      "Manoeuvring 1\n",
      "Dogfighting 1\n",
      "gives 1\n",
      "By 1\n",
      "end 2\n",
      "12 2\n",
      "tease 1\n",
      "Blinds 1\n",
      "please 2\n",
      "Our 1\n",
      "commanding 1\n",
      "officer 1\n",
      "veryfirst 1\n",
      "win 1\n",
      "trophy 2\n",
      "wo 7\n",
      "find 3\n",
      "finer 1\n",
      "fighter 2\n",
      "pilotanywhere 1\n",
      "Commander 2\n",
      "Mike 2\n",
      "Metcalf 1\n",
      "sign 2\n",
      "Viper 11\n",
      "... 48\n",
      "top 1\n",
      "percentof 1\n",
      "naval 2\n",
      "aviators 1\n",
      "elite 1\n",
      "We'll 1\n",
      "better 4\n",
      "least 2\n",
      "combat 3\n",
      "missionsa 1\n",
      "attend 1\n",
      "classes 1\n",
      "evaluations 1\n",
      "On 3\n",
      "each 2\n",
      "sequence 1\n",
      "you'regonna 1\n",
      "meet 1\n",
      "different 1\n",
      "challenge 1\n",
      "F-14faster 1\n",
      "ever 4\n",
      "flown 2\n",
      "Now 3\n",
      "policy 1\n",
      "Elected 1\n",
      "officials 1\n",
      "civilians 1\n",
      "instruments 1\n",
      "mustalways 1\n",
      "act 3\n",
      "though 3\n",
      "war 1\n",
      "wondering 1\n",
      "who 6\n",
      "In 2\n",
      "case 2\n",
      "wonderwho 1\n",
      "plaque 3\n",
      "driver 2\n",
      "RIOfrom 1\n",
      "class 2\n",
      "name 1\n",
      "They 12\n",
      "option 2\n",
      "hereand 1\n",
      "instructors 2\n",
      "will 8\n",
      "pretty 1\n",
      "arrogant 1\n",
      "considering 1\n",
      "company 1\n",
      "Remember 2\n",
      "when 6\n",
      "we're 1\n",
      "same 1\n",
      "team 2\n",
      "school 2\n",
      "points 4\n",
      "second 6\n",
      "place 2\n",
      "Dismissed 2\n",
      "Nice 1\n",
      "alternatesis 1\n",
      "ladies 2\n",
      "room 2\n",
      "Jesus 6\n",
      "kill 3\n",
      "O 1\n",
      "target 1\n",
      "rich 1\n",
      "environment 1\n",
      "live 2\n",
      "life 2\n",
      "between 2\n",
      "legs 1\n",
      "Even 2\n",
      "could 13\n",
      "laid 1\n",
      "'d 9\n",
      "happy 2\n",
      "girlwho'd 1\n",
      "talk 1\n",
      "dirty 1\n",
      "wanted 3\n",
      "is?That 1\n",
      "Iceman 2\n",
      "flies 1\n",
      "ice 1\n",
      "cold 1\n",
      "mistakes 2\n",
      "Wears 1\n",
      "bored 1\n",
      "stupid 2\n",
      "thought 3\n",
      "Whose 1\n",
      "kiss 1\n",
      "list 1\n",
      "long 7\n",
      "distinguished 1\n",
      "So 11\n",
      "myjohnson 1\n",
      "flying 8\n",
      "lceman 4\n",
      "Hey 4\n",
      "Mother 1\n",
      "how 3\n",
      "Tom 2\n",
      "Pete 4\n",
      "Mitchell 5\n",
      "Kazansky 2\n",
      "Congratulations 4\n",
      "Sorry 5\n",
      "tohear 1\n",
      "Still 2\n",
      "Yeah 7\n",
      "what 15\n",
      "meant 1\n",
      "any 6\n",
      "help?You 1\n",
      "figured 3\n",
      "yet 3\n",
      "figure 2\n",
      "own 2\n",
      "heard 6\n",
      "work 3\n",
      "First 1\n",
      "MiGand 1\n",
      "then 5\n",
      "slide 2\n",
      "into 5\n",
      "spot 2\n",
      "Some 2\n",
      "wait 1\n",
      "whole 1\n",
      "career 2\n",
      "seea 1\n",
      "close 5\n",
      "Lucky 1\n",
      "famous 2\n",
      "mean 3\n",
      "notorious 1\n",
      "See 1\n",
      "later 1\n",
      "count 4\n",
      "abused 1\n",
      "children 1\n",
      "guess 3\n",
      "turn 3\n",
      "it?All 1\n",
      "bet 3\n",
      "20 3\n",
      "dollars 1\n",
      "carnal 1\n",
      "knowledge 1\n",
      "lady 1\n",
      "premises 2\n",
      "does 10\n",
      "seem 2\n",
      "fair 1\n",
      "She 6\n",
      "loving 1\n",
      "feeling 2\n",
      "she 2\n",
      "hate 3\n",
      "Excuse 5\n",
      "miss 1\n",
      "worry 2\n",
      "care 2\n",
      "Sit 1\n",
      "love 4\n",
      "song 1\n",
      "new 2\n",
      "approach 1\n",
      "How 8\n",
      "been 8\n",
      "Since 1\n",
      "Puberty 1\n",
      "Charlotte 1\n",
      "Blackwood 1\n",
      "Did 3\n",
      "mother 2\n",
      "aviator 1\n",
      "Actually 1\n",
      "only 5\n",
      "Crashed 1\n",
      "burned 2\n",
      "first 6\n",
      "And 7\n",
      "tell 7\n",
      "tomorrow 2\n",
      "looking 4\n",
      "far 3\n",
      "Well 5\n",
      "friend 3\n",
      "arrived 1\n",
      "great 5\n",
      "talking 1\n",
      "Perry 1\n",
      "Can 2\n",
      "ask 2\n",
      "personal 2\n",
      "question 1\n",
      "depends 1\n",
      "hold 1\n",
      "Then 3\n",
      "aboutyou 1\n",
      "making 1\n",
      "living 1\n",
      "singer 1\n",
      "beer 1\n",
      "put 1\n",
      "theseflames 1\n",
      "Real 1\n",
      "slick 1\n",
      "cruise 1\n",
      "sailor 1\n",
      "Too 2\n",
      "drop 1\n",
      "tileand 1\n",
      "actually 1\n",
      "counter 1\n",
      "very 1\n",
      "comfortable 2\n",
      "yeah 1\n",
      "came 1\n",
      "save 2\n",
      "from 4\n",
      "makinga 1\n",
      "big 9\n",
      "mistake 1\n",
      "older 1\n",
      "Really 2\n",
      "bigger 1\n",
      "onewith 2\n",
      "young 1\n",
      "yourself 2\n",
      "early 2\n",
      "morning 2\n",
      "magnificent 1\n",
      "also 1\n",
      "trained 2\n",
      "evaluatedby 1\n",
      "civilian 1\n",
      "specialists 1\n",
      "source 1\n",
      "informationon 1\n",
      "enemy 3\n",
      "One 7\n",
      "most 2\n",
      "qualified 1\n",
      "signCharlie 1\n",
      "Ph.D. 1\n",
      "astrophysics 1\n",
      "Listen 1\n",
      "her 7\n",
      "because 2\n",
      "Pentagonlistens 1\n",
      "proficiency 1\n",
      "All 5\n",
      "yours 2\n",
      "Charlie 3\n",
      "Hello 2\n",
      "dealing 1\n",
      "F-5sand 1\n",
      "A-4s 2\n",
      "simulators 1\n",
      "As 2\n",
      "F-5 1\n",
      "havethe 1\n",
      "thrust 1\n",
      "weight 1\n",
      "MiG-28 6\n",
      "bleed 1\n",
      "energybelow 1\n",
      "However 1\n",
      "havea 1\n",
      "its 1\n",
      "inverted 3\n",
      "flight 2\n",
      "tanks 1\n",
      "negative 2\n",
      "G 3\n",
      "push 3\n",
      "latest 1\n",
      "intelligence 1\n",
      "tells 3\n",
      "usthe 1\n",
      "Lieutenant 7\n",
      "wrong 3\n",
      "data 1\n",
      "inaccurate 1\n",
      "happened 4\n",
      "MiG-28do 1\n",
      "four 5\n",
      "dive 1\n",
      "Where 10\n",
      "classified 3\n",
      "Secret 1\n",
      "clearance 2\n",
      "Pentagon 1\n",
      "sees 1\n",
      "itthat 1\n",
      "more 7\n",
      "where 2\n",
      "exactly 1\n",
      "started 1\n",
      "six 2\n",
      "pulledthrough 1\n",
      "clouds 1\n",
      "went 3\n",
      "above 3\n",
      "directly 1\n",
      "Because 1\n",
      "Bullshit 3\n",
      "move 3\n",
      "divewith 1\n",
      "ma'am 2\n",
      "At 2\n",
      "About 1\n",
      "metres 1\n",
      "11/2 1\n",
      "Polaroid 1\n",
      "nice 1\n",
      "picture 1\n",
      "Communicating 1\n",
      "Keeping 1\n",
      "foreign 2\n",
      "relations 1\n",
      "giving 1\n",
      "bird 1\n",
      "finger 2\n",
      "hop 4\n",
      "10,000 1\n",
      "feet 2\n",
      "There'll 1\n",
      "engagement 3\n",
      "Move 1\n",
      "late 3\n",
      "again 3\n",
      "look 4\n",
      "honey 2\n",
      "Why 3\n",
      "meyou 1\n",
      "lnsulter 1\n",
      "Would 1\n",
      "made 2\n",
      "difference 1\n",
      "Not 1\n",
      "hotshots 1\n",
      "eightweeks 1\n",
      "sure 1\n",
      "liketo 1\n",
      "sometime 1\n",
      "security 2\n",
      "read 1\n",
      "Maverick?.I'm 1\n",
      "curious 1\n",
      "covering 1\n",
      "while 3\n",
      "youwere 1\n",
      "showboating 1\n",
      "fine 1\n",
      "fiirsthop 1\n",
      "Thejets 1\n",
      "you'reflying 1\n",
      "smaller 1\n",
      "faster 1\n",
      "-justlike 1\n",
      "MiGs 6\n",
      "ofnow 1\n",
      "keeping 1\n",
      "score 2\n",
      "Show 2\n",
      "Here 3\n",
      "still 9\n",
      "Closing 1\n",
      "run 2\n",
      "hide 1\n",
      "Jester 10\n",
      "I'll 1\n",
      "money 1\n",
      "mountains 1\n",
      "bring 2\n",
      "hit 11\n",
      "brakes 2\n",
      "mine 3\n",
      "Christ 4\n",
      "burn 2\n",
      "vertical 1\n",
      "am 3\n",
      "l. 2\n",
      "ballistic 2\n",
      "nail 1\n",
      "dead 10\n",
      "butts 5\n",
      "deckand 1\n",
      "return 2\n",
      "base 1\n",
      "requesting 2\n",
      "pattern 2\n",
      "buzz 1\n",
      "tower 1\n",
      "son 5\n",
      "bitch 4\n",
      "balls 1\n",
      "won 4\n",
      "everybody 1\n",
      "kicked 1\n",
      "said 4\n",
      "Hollywood 11\n",
      "go?\"He 1\n",
      "laughing 1\n",
      "radio 1\n",
      "dickhead 1\n",
      "Below 1\n",
      "nailed 1\n",
      "cowboys 1\n",
      "everyone 1\n",
      "Every 1\n",
      "unsafe 1\n",
      "dangerous 6\n",
      "Gooseget 1\n",
      "office 1\n",
      "lot 5\n",
      "brighter 1\n",
      "shut 1\n",
      "Two 4\n",
      "snot 1\n",
      "nosed 1\n",
      "jockeys 1\n",
      "flew 4\n",
      "byby 1\n",
      "400 1\n",
      "Damn 4\n",
      "covers 1\n",
      "bys 1\n",
      "Follow 1\n",
      "was10,000 1\n",
      "knew 2\n",
      "broke 3\n",
      "followed 1\n",
      "Heatherlybelow 1\n",
      "sight 1\n",
      "Sir 3\n",
      "sights 1\n",
      "saw 2\n",
      "proceeded 1\n",
      "forjust 1\n",
      "few 2\n",
      "seconds 7\n",
      "danger 1\n",
      "took 2\n",
      "major 1\n",
      "rule 1\n",
      "circus 1\n",
      "stunt 1\n",
      "rules 1\n",
      "existfor 1\n",
      "safety 1\n",
      "flexible 1\n",
      "nor 1\n",
      "Obey 1\n",
      "themor 1\n",
      "enjoyed 2\n",
      "thanks 1\n",
      "Holy 2\n",
      "truck 2\n",
      "gotthe 1\n",
      "driving 1\n",
      "Fitness 1\n",
      "report 1\n",
      "says 1\n",
      "wild 1\n",
      "card 1\n",
      "Completely 1\n",
      "unpredictable 1\n",
      "old 6\n",
      "battle 2\n",
      "awake 1\n",
      "sleep 1\n",
      "When 2\n",
      "realised 1\n",
      "straight 2\n",
      "Right 3\n",
      "hope 1\n",
      "graduate 1\n",
      "afford 1\n",
      "blow 3\n",
      "bywasn't 1\n",
      "such 1\n",
      "huh 2\n",
      "tough 2\n",
      "Academy 1\n",
      "rejectedyou 1\n",
      "Duke 1\n",
      "reputation 1\n",
      "ghost 1\n",
      "makes 3\n",
      "nervous 1\n",
      "promise 1\n",
      "rolling 1\n",
      "reversalwould 1\n",
      "well 2\n",
      "situation 2\n",
      "reversed 1\n",
      "cross 1\n",
      "guns 3\n",
      "fast 2\n",
      "bit 1\n",
      "aggressive 3\n",
      "chance 3\n",
      "deserved 1\n",
      "temptedto 1\n",
      "dinner 1\n",
      "date 1\n",
      "students 1\n",
      "government 1\n",
      "trusts 1\n",
      "maybe 3\n",
      "takes 1\n",
      "fancy 1\n",
      "Crash 1\n",
      "Slider 2\n",
      "stink 2\n",
      "game 3\n",
      "evens 1\n",
      "things 1\n",
      "Please 2\n",
      "me?Come 1\n",
      "singing 1\n",
      "Look 3\n",
      "apologies 2\n",
      "quickshower 1\n",
      "finishing 1\n",
      "hungry 1\n",
      "thought,\"You 1\n",
      "he\\ 1\n",
      "smart 1\n",
      "don\\'t 1\n",
      "himwhy 1\n",
      "you\\'ve 1\n",
      "MiG. 3\n",
      "pilotwho 1\n",
      "trying 1\n",
      "promotion 1\n",
      "much 1\n",
      "longer 1\n",
      "Seems 1\n",
      "Except 1\n",
      "forget 1\n",
      "wine 1\n",
      "always 3\n",
      "relax 1\n",
      "music 1\n",
      "An 1\n",
      "folks 1\n",
      "loved 2\n",
      "years 2\n",
      "used 1\n",
      "roomand 1\n",
      "play 1\n",
      "sick 1\n",
      "sit 1\n",
      "listening 2\n",
      "hours 1\n",
      "died 2\n",
      "shortly 1\n",
      "father 1\n",
      "clearanceyou'd 1\n",
      "mystery 1\n",
      "disappeared 1\n",
      "F-4,November 1\n",
      "5th 1\n",
      "1965 1\n",
      "knows?It 1\n",
      "Somebody 1\n",
      "must 1\n",
      "knows 1\n",
      "everything 1\n",
      "whyyou're 1\n",
      "direct 2\n",
      "being 3\n",
      "complicated 2\n",
      "shower 2\n",
      "looks 1\n",
      "honest 1\n",
      "sightingis 1\n",
      "important 1\n",
      "normally 1\n",
      "invite 1\n",
      "studentsto 1\n",
      "house 1\n",
      "understand 2\n",
      "job 2\n",
      "glad 1\n",
      "told 3\n",
      "ass 2\n",
      "dragging 1\n",
      "tired 1\n",
      "woman 1\n",
      "probablydoesn\\'t 1\n",
      "eight 2\n",
      "Hi 3\n",
      "Daddy 1\n",
      "lovewith 1\n",
      "believeyou 1\n",
      "secret 1\n",
      "Freeze 2\n",
      "frame 1\n",
      "moment 1\n",
      "choice 3\n",
      "F-14 1\n",
      "defensive 2\n",
      "bug 1\n",
      "Better 1\n",
      "retire 1\n",
      "aircraftthan 1\n",
      "bad 2\n",
      "jump 1\n",
      "stayin 1\n",
      "diamond 1\n",
      "away 3\n",
      "select 1\n",
      "zone 1\n",
      "extend 1\n",
      "escape 1\n",
      "Aircraft 1\n",
      "performs 1\n",
      "split 1\n",
      "S 1\n",
      "last 2\n",
      "inhis 1\n",
      "gunsight 1\n",
      "thinking 1\n",
      "gamblewith 1\n",
      "$ 1\n",
      "30 3\n",
      "million 1\n",
      "Unfortunately 1\n",
      "worked 1\n",
      "verticalmove 1\n",
      "defeats 1\n",
      "encounter 1\n",
      "victory 1\n",
      "weshow 1\n",
      "example 1\n",
      "Next 1\n",
      "Gutsiest 1\n",
      "exampleof 1\n",
      "textbook 1\n",
      "manoeuvre 2\n",
      "review 1\n",
      "performance 2\n",
      "wasright 1\n",
      "professional 1\n",
      "opinion 1\n",
      "reckless 1\n",
      "WhenI 1\n",
      "crew 1\n",
      "finish 1\n",
      "sentence 1\n",
      "reviewof 1\n",
      "real 2\n",
      "genius 1\n",
      "say 3\n",
      "afraid 1\n",
      "through 2\n",
      "anyone 1\n",
      "knowthat 1\n",
      "fallen 1\n",
      "Hear 1\n",
      "lce 7\n",
      "19 1\n",
      "Multiple 2\n",
      "multiplebogeys 1\n",
      "training 1\n",
      "halfover 1\n",
      "competltion 1\n",
      "trophyremains 1\n",
      "Firstplace 1\n",
      "Second 1\n",
      "behind 3\n",
      "Three 5\n",
      "o'clock 7\n",
      "Left 1\n",
      "level 1\n",
      "Continue 1\n",
      "temperature 1\n",
      "Oh 5\n",
      "He\\ 1\n",
      "probably 1\n",
      "saying,\"Holy 1\n",
      "it\\ 1\n",
      "eye 1\n",
      "northern 1\n",
      "southern 1\n",
      "lead 3\n",
      "cover 2\n",
      "Break 4\n",
      "losing 2\n",
      "Stay 1\n",
      "leave 4\n",
      "Rock 1\n",
      "roll 1\n",
      "goddamn 1\n",
      "keep 1\n",
      "sidewinder 1\n",
      "selected 1\n",
      "Keep 2\n",
      "switching 2\n",
      "Stick 1\n",
      "Hold 1\n",
      "Check 2\n",
      "You're 1\n",
      "Defence 1\n",
      "Department 1\n",
      "regrets 1\n",
      "toinform 1\n",
      "sons 1\n",
      "stupidity 1\n",
      "Knock 1\n",
      "Wolfman 3\n",
      "Shut 1\n",
      "flyingI've 1\n",
      "Until 1\n",
      "killed 1\n",
      "wingman 3\n",
      "attitude 1\n",
      "nowyou're 1\n",
      "worse 2\n",
      "Dangerous 1\n",
      "foolish 1\n",
      "may 1\n",
      "whose 1\n",
      "side 1\n",
      "beforehe 1\n",
      "happen 1\n",
      "fetch 1\n",
      "Doesn't 1\n",
      "embarrass 2\n",
      "Hell 1\n",
      "Admiral 1\n",
      "timeyou 1\n",
      "angelGoose 1\n",
      "goes 1\n",
      "church 1\n",
      "hot 1\n",
      "women 1\n",
      "Carole 1\n",
      "myselfwith 1\n",
      "able 1\n",
      "warn 1\n",
      "aboutMaverick 1\n",
      "death 1\n",
      "known 1\n",
      "certain 1\n",
      "hearts 1\n",
      "breaking 1\n",
      "wide 1\n",
      "openall 1\n",
      "world 2\n",
      "tonight 1\n",
      "Unless 1\n",
      "fool 1\n",
      "boy 1\n",
      "market 1\n",
      "100%%% 1\n",
      "prime 1\n",
      "stud 2\n",
      "bed 2\n",
      "or 4\n",
      "lose 2\n",
      "forever 2\n",
      "brother 1\n",
      "sing 1\n",
      "31.Two 1\n",
      "graduation 1\n",
      "trophyis 1\n",
      "forgrabs 1\n",
      "tied 1\n",
      "Time 1\n",
      "Contact 1\n",
      "bogeys 3\n",
      "Looks 1\n",
      "cut 1\n",
      "angle 1\n",
      "Fire 2\n",
      "moving 1\n",
      "impatient 1\n",
      "Five 1\n",
      "jetwash 3\n",
      "flame 1\n",
      "Engine 2\n",
      "coupling 1\n",
      "Mayday 1\n",
      "mayday 1\n",
      "flat 1\n",
      "spin 4\n",
      "sea 1\n",
      "Altitude 1\n",
      "8,000 1\n",
      "7,000 1\n",
      "6,000 1\n",
      "reach 2\n",
      "ejection 2\n",
      "handle 2\n",
      "punch 1\n",
      "Eject 1\n",
      "canopy 1\n",
      "God 5\n",
      "You've 2\n",
      "flyjets 1\n",
      "happens 1\n",
      "RIO 1\n",
      "responsibility 2\n",
      "squadron 1\n",
      "8 1\n",
      "18 1\n",
      "men 1\n",
      "dies 1\n",
      "die 1\n",
      "others 1\n",
      "fault 3\n",
      "help 6\n",
      "difficult 1\n",
      "anyway 1\n",
      "without 2\n",
      "hated 1\n",
      "induced 1\n",
      "disruptionof 1\n",
      "flow 1\n",
      "starboard 1\n",
      "engine 3\n",
      "disruption 1\n",
      "stalled 1\n",
      "which 2\n",
      "produced 1\n",
      "yaw 1\n",
      "rateto 1\n",
      "induce 1\n",
      "unrecoverable 1\n",
      "Mitchellcould 1\n",
      "avoid 1\n",
      "Board 1\n",
      "lnquiry 1\n",
      "finds 1\n",
      "wasnot 1\n",
      "accident 1\n",
      "29 1\n",
      "July 1\n",
      "His 2\n",
      "record 1\n",
      "cleared 1\n",
      "restoredto 1\n",
      "status 1\n",
      "further 1\n",
      "delay 1\n",
      "These 1\n",
      "proceedings 1\n",
      "closed 1\n",
      "Engage 2\n",
      "getto 1\n",
      "goddamngood 1\n",
      "ready 3\n",
      "days 1\n",
      "might 1\n",
      "sending 1\n",
      "Everybody 1\n",
      "liked 1\n",
      "Maverickjust 1\n",
      "quit 4\n",
      "May 1\n",
      "l 1\n",
      "having 1\n",
      "Hemlock 1\n",
      "Ice 6\n",
      "water 1\n",
      "goodbye 2\n",
      "Washington 1\n",
      "leavewithout 1\n",
      "saying 1\n",
      "going?You 1\n",
      "ticket 1\n",
      "evidence 1\n",
      "notyour 1\n",
      "met 1\n",
      "larger 1\n",
      "unless 1\n",
      "you'regoing 1\n",
      "Mach 1\n",
      "2 1\n",
      "hair 1\n",
      "To 1\n",
      "means 1\n",
      "youmake 1\n",
      "thank 1\n",
      "asked 1\n",
      "already 1\n",
      "learn 1\n",
      "damn 1\n",
      "you?Except 1\n",
      "downreal 1\n",
      "you?Sure 1\n",
      "Jamie 1\n",
      "drink 1\n",
      "Make 2\n",
      "VF-51 1\n",
      "Oriskany 1\n",
      "Only 1\n",
      "natural 1\n",
      "heroic 1\n",
      "why 1\n",
      "do?Trying 1\n",
      "prove 1\n",
      "worst 1\n",
      "dogfight 1\n",
      "Bogeys 1\n",
      "fireflies 1\n",
      "sky 1\n",
      "F-4 1\n",
      "wounded 1\n",
      "stayed 1\n",
      "Saved 1\n",
      "planes 1\n",
      "bought 1\n",
      "occurredover 1\n",
      "line 1\n",
      "map 1\n",
      "options 1\n",
      "Simple 1\n",
      "acquired 1\n",
      "show 1\n",
      "graduatewith 1\n",
      "Or 1\n",
      "disgrace 1\n",
      "feel 1\n",
      "responsible 1\n",
      "confidence 1\n",
      "sunshine 1\n",
      "compelledto 1\n",
      "evaluate 1\n",
      "apply 1\n",
      "learned 1\n",
      "Up 1\n",
      "we've 1\n",
      "ourjob 1\n",
      "bother 1\n",
      "Sunday 1\n",
      "planning 1\n",
      "break 3\n",
      "partybefore 1\n",
      "gets 1\n",
      "hand 1\n",
      "departimmediately 1\n",
      "crisis 1\n",
      "RIOwhen 1\n",
      "SS 1\n",
      "Layton 1\n",
      "become 1\n",
      "disabledand 1\n",
      "wandered 1\n",
      "territory 1\n",
      "rescue 2\n",
      "operation 1\n",
      "begin 1\n",
      "mission 1\n",
      "support 1\n",
      "area 1\n",
      "youwitness 1\n",
      "hostile 1\n",
      "carry 1\n",
      "Exocet 1\n",
      "anti 1\n",
      "100 1\n",
      "proud 1\n",
      "sector 1\n",
      "Merlinon 1\n",
      "Rise 1\n",
      ".. 1\n",
      "pair 1\n",
      "bogeys,12 1\n",
      "15 2\n",
      "Voodoo 4\n",
      "contact15 1\n",
      "500 1\n",
      "Dead 2\n",
      "ahead 6\n",
      "600 1\n",
      "15,000 1\n",
      "feetnow 1\n",
      "800 1\n",
      "Wood 5\n",
      "mustbe 1\n",
      "taking 2\n",
      "identify 1\n",
      "leftside 1\n",
      "Mybogey 1\n",
      "drifting 1\n",
      "Stillmanoeuvring 1\n",
      "course 1\n",
      "fouraircraft 1\n",
      "Four 1\n",
      "Wrong 1\n",
      "apart 1\n",
      "We've 1\n",
      "lostHollywood 1\n",
      "Launch 3\n",
      "andWolfman 1\n",
      "helicopter 1\n",
      "bringing 2\n",
      "alertfiighters 1\n",
      "160 1\n",
      "closing 1\n",
      "Ready 1\n",
      "Willard 2\n",
      "Simkin 2\n",
      "engaged 2\n",
      "deep 1\n",
      "minutes?It 1\n",
      "minutes 1\n",
      "getyourbutt 1\n",
      "Hang 1\n",
      "recovery 1\n",
      "disengaging 1\n",
      "re 1\n",
      "tone 2\n",
      "Firing 3\n",
      "155 1\n",
      "overshot 1\n",
      "Coming 2\n",
      "passing 1\n",
      "north 1\n",
      "Banking 1\n",
      "round 1\n",
      "leaving 1\n",
      "goingfor 1\n",
      "shutting 1\n",
      "manoeuvring 1\n",
      "Shoot 1\n",
      "missed 1\n",
      "comes 1\n",
      "Strike 2\n",
      "slowing 1\n",
      "closer 1\n",
      "Remaining 1\n",
      "anytime 1\n",
      "front 1\n",
      "pagesof 1\n",
      "English 1\n",
      "speaking 1\n",
      "sidedenies 1\n",
      "incident 1\n",
      "congratulations 1\n",
      "gave 1\n",
      "duty 1\n",
      "instructor 1\n",
      "bestwere 1\n",
      "crashed 1\n",
      "THE 1\n",
      "END 1\n",
      "Subtitles 1\n",
      "Skimaniac 1\n",
      "] 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "  \n",
    "# Creation of a Counter Class object using  \n",
    "# string as an iterable data container \n",
    "#c = Counter(tokens) \n",
    "  \n",
    "# printing the elements of counter object\n",
    "#for key, value in c.items():\n",
    "#    print(key, value)\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    pass\n",
    "\n",
    "c = OrderedCounter(tokens)\n",
    "for key, value in c.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1065),\n",
       " ('.', 986),\n",
       " (\"'\", 726),\n",
       " ('\"', 717),\n",
       " ('I', 251),\n",
       " ('-', 233),\n",
       " (\"'s\", 170),\n",
       " ('you', 166),\n",
       " ('the', 155),\n",
       " ('?', 131),\n",
       " ('a', 125),\n",
       " ('to', 110),\n",
       " ('You', 98),\n",
       " ('it', 94),\n",
       " ('on', 90),\n",
       " ('!', 81),\n",
       " (\"n't\", 78),\n",
       " ('in', 67),\n",
       " ('got', 63),\n",
       " ('of', 61),\n",
       " (\"'re\", 59),\n",
       " (\"'m\", 54),\n",
       " ('that', 53),\n",
       " ('your', 52),\n",
       " ('is', 51),\n",
       " ('...', 48),\n",
       " ('him', 47),\n",
       " ('He', 47),\n",
       " ('me', 45),\n",
       " (\"'ve\", 45),\n",
       " ('We', 43),\n",
       " ('have', 42),\n",
       " ('do', 41),\n",
       " ('was', 41),\n",
       " ('and', 39),\n",
       " ('right', 37),\n",
       " ('up', 36),\n",
       " ('Maverick', 36),\n",
       " ('be', 35),\n",
       " ('for', 34),\n",
       " ('there', 33),\n",
       " ('The', 32),\n",
       " ('he', 31),\n",
       " ('with', 31),\n",
       " ('are', 31),\n",
       " ('know', 29),\n",
       " ('out', 27),\n",
       " ('That', 27),\n",
       " ('MiG', 26),\n",
       " ('one', 26),\n",
       " ('this', 26),\n",
       " (\"'ll\", 25),\n",
       " ('sir', 25),\n",
       " ('here', 25),\n",
       " ('my', 25),\n",
       " ('It', 24),\n",
       " ('What', 23),\n",
       " ('Cougar', 22),\n",
       " ('get', 22),\n",
       " ('This', 22),\n",
       " ('did', 21),\n",
       " ('about', 21),\n",
       " ('we', 20),\n",
       " ('see', 20),\n",
       " ('go', 20),\n",
       " ('not', 20),\n",
       " ('Come', 20),\n",
       " ('just', 20),\n",
       " ('Goose', 19),\n",
       " ('going', 19),\n",
       " ('ca', 19),\n",
       " ('good', 19),\n",
       " ('them', 18),\n",
       " ('best', 18),\n",
       " ('like', 17),\n",
       " ('all', 17),\n",
       " ('No', 17),\n",
       " ('were', 17),\n",
       " ('miles', 16),\n",
       " ('gon', 16),\n",
       " ('na', 16),\n",
       " ('can', 16),\n",
       " ('shot', 16),\n",
       " ('two', 15),\n",
       " ('Let', 15),\n",
       " ('but', 15),\n",
       " ('But', 15),\n",
       " ('what', 15),\n",
       " ('at', 14),\n",
       " ('us', 14),\n",
       " ('Get', 14),\n",
       " ('tail', 14),\n",
       " ('hard', 14),\n",
       " ('fly', 14),\n",
       " ('Mav', 13),\n",
       " ('over', 13),\n",
       " ('There', 13),\n",
       " ('man', 13),\n",
       " ('down', 13),\n",
       " ('could', 13),\n",
       " ('They', 12),\n",
       " ('left', 11),\n",
       " ('coming', 11),\n",
       " ('lost', 11),\n",
       " ('our', 11),\n",
       " ('back', 11),\n",
       " ('has', 11),\n",
       " ('think', 11),\n",
       " ('time', 11),\n",
       " ('Viper', 11),\n",
       " ('So', 11),\n",
       " ('hit', 11),\n",
       " ('Hollywood', 11),\n",
       " ('an', 10),\n",
       " ('10', 10),\n",
       " ('Do', 10),\n",
       " ('doing', 10),\n",
       " ('off', 10),\n",
       " ('come', 10),\n",
       " ('take', 10),\n",
       " ('no', 10),\n",
       " ('too', 10),\n",
       " ('My', 10),\n",
       " ('hell', 10),\n",
       " ('want', 10),\n",
       " ('pilot', 10),\n",
       " ('Top', 10),\n",
       " ('does', 10),\n",
       " ('Where', 10),\n",
       " ('Jester', 10),\n",
       " ('dead', 10),\n",
       " ('bogey', 9),\n",
       " ('now', 9),\n",
       " ('If', 9),\n",
       " ('five', 9),\n",
       " ('way', 9),\n",
       " ('by', 9),\n",
       " ('so', 9),\n",
       " ('low', 9),\n",
       " ('than', 9),\n",
       " ('Gun', 9),\n",
       " ('his', 9),\n",
       " ('shit', 9),\n",
       " ('Yes', 9),\n",
       " ('some', 9),\n",
       " (\"'d\", 9),\n",
       " ('big', 9),\n",
       " ('still', 9),\n",
       " ('Great', 8),\n",
       " ('Roger', 8),\n",
       " ('Take', 8),\n",
       " ('really', 8),\n",
       " ('Shit', 8),\n",
       " ('fire', 8),\n",
       " ('had', 8),\n",
       " ('Just', 8),\n",
       " ('A', 8),\n",
       " ('kid', 8),\n",
       " ('guy', 8),\n",
       " ('will', 8),\n",
       " ('flying', 8),\n",
       " ('How', 8),\n",
       " ('been', 8),\n",
       " ('knots', 7),\n",
       " ('mind', 7),\n",
       " ('after', 7),\n",
       " ('Goddamn', 7),\n",
       " ('they', 7),\n",
       " ('let', 7),\n",
       " ('deck', 7),\n",
       " ('thing', 7),\n",
       " ('call', 7),\n",
       " ('need', 7),\n",
       " ('problem', 7),\n",
       " ('something', 7),\n",
       " ('wo', 7),\n",
       " ('long', 7),\n",
       " ('Yeah', 7),\n",
       " ('And', 7),\n",
       " ('tell', 7),\n",
       " ('One', 7),\n",
       " ('her', 7),\n",
       " ('Lieutenant', 7),\n",
       " ('more', 7),\n",
       " ('seconds', 7),\n",
       " ('lce', 7),\n",
       " (\"o'clock\", 7),\n",
       " ('if', 6),\n",
       " ('position', 6),\n",
       " (\"I'm\", 6),\n",
       " ('aircraft', 6),\n",
       " ('missile', 6),\n",
       " ('home', 6),\n",
       " ('would', 6),\n",
       " ('little', 6),\n",
       " ('Is', 6),\n",
       " ('Watch', 6),\n",
       " ('as', 6),\n",
       " ('never', 6),\n",
       " ('sorry', 6),\n",
       " ('Your', 6),\n",
       " ('Thank', 6),\n",
       " ('who', 6),\n",
       " ('when', 6),\n",
       " ('second', 6),\n",
       " ('Jesus', 6),\n",
       " ('any', 6),\n",
       " ('heard', 6),\n",
       " ('She', 6),\n",
       " ('first', 6),\n",
       " ('MiG-28', 6),\n",
       " ('MiGs', 6),\n",
       " ('dangerous', 6),\n",
       " ('old', 6),\n",
       " ('help', 6),\n",
       " ('Ice', 6),\n",
       " ('ahead', 6),\n",
       " ('radar', 5),\n",
       " ('their', 5),\n",
       " ('lock', 5),\n",
       " ('should', 5),\n",
       " ('enough', 5),\n",
       " ('seen', 5),\n",
       " ('make', 5),\n",
       " ('okay', 5),\n",
       " ('below', 5),\n",
       " ('three', 5),\n",
       " ('control', 5),\n",
       " ('another', 5),\n",
       " ('against', 5),\n",
       " ('Gentlemen', 5),\n",
       " ('Mitchell', 5),\n",
       " ('Sorry', 5),\n",
       " ('then', 5),\n",
       " ('into', 5),\n",
       " ('close', 5),\n",
       " ('Excuse', 5),\n",
       " ('only', 5),\n",
       " ('Well', 5),\n",
       " ('great', 5),\n",
       " ('All', 5),\n",
       " ('four', 5),\n",
       " ('butts', 5),\n",
       " ('son', 5),\n",
       " ('lot', 5),\n",
       " ('Three', 5),\n",
       " ('Oh', 5),\n",
       " ('God', 5),\n",
       " ('Wood', 5),\n",
       " ('090', 4),\n",
       " ('Merlin', 4),\n",
       " ('Are', 4),\n",
       " ('head', 4),\n",
       " ('alone', 4),\n",
       " ('Okay', 4),\n",
       " ('around', 4),\n",
       " ('firing', 4),\n",
       " ('getting', 4),\n",
       " ('land', 4),\n",
       " ('before', 4),\n",
       " ('Thanks', 4),\n",
       " ('plane', 4),\n",
       " ('family', 4),\n",
       " ('give', 4),\n",
       " ('Good', 4),\n",
       " ('1', 4),\n",
       " ('better', 4),\n",
       " ('ever', 4),\n",
       " ('points', 4),\n",
       " ('lceman', 4),\n",
       " ('Hey', 4),\n",
       " ('Pete', 4),\n",
       " ('Congratulations', 4),\n",
       " ('count', 4),\n",
       " ('love', 4),\n",
       " ('looking', 4),\n",
       " ('from', 4),\n",
       " ('happened', 4),\n",
       " ('hop', 4),\n",
       " ('look', 4),\n",
       " ('Christ', 4),\n",
       " ('bitch', 4),\n",
       " ('won', 4),\n",
       " ('said', 4),\n",
       " ('Two', 4),\n",
       " ('flew', 4),\n",
       " ('Damn', 4),\n",
       " ('Break', 4),\n",
       " ('leave', 4),\n",
       " ('or', 4),\n",
       " ('spin', 4),\n",
       " ('quit', 4),\n",
       " ('Voodoo', 4),\n",
       " ('Ghost', 3),\n",
       " ('Rider', 3),\n",
       " ('Who', 3),\n",
       " ('Talk', 3),\n",
       " ('hear', 3),\n",
       " ('Negative', 3),\n",
       " ('buddy', 3),\n",
       " ('Go', 3),\n",
       " ('baby', 3),\n",
       " ('Bingo', 3),\n",
       " ('engage', 3),\n",
       " ('clear', 3),\n",
       " ('day', 3),\n",
       " ('gas', 3),\n",
       " ('ship', 3),\n",
       " ('soon', 3),\n",
       " ('ball', 3),\n",
       " ('Tell', 3),\n",
       " ('even', 3),\n",
       " ('done', 3),\n",
       " ('twice', 3),\n",
       " ('speed', 3),\n",
       " ('air', 3),\n",
       " ('other', 3),\n",
       " ('Maybe', 3),\n",
       " ('For', 3),\n",
       " ('number', 3),\n",
       " ('guys', 3),\n",
       " ('full', 3),\n",
       " ('Vietnam', 3),\n",
       " ('pilots', 3),\n",
       " ('find', 3),\n",
       " ('combat', 3),\n",
       " ('On', 3),\n",
       " ('Now', 3),\n",
       " ('act', 3),\n",
       " ('though', 3),\n",
       " ('plaque', 3),\n",
       " ('kill', 3),\n",
       " ('wanted', 3),\n",
       " ('thought', 3),\n",
       " ('how', 3),\n",
       " ('figured', 3),\n",
       " ('yet', 3),\n",
       " ('work', 3),\n",
       " ('mean', 3),\n",
       " ('guess', 3),\n",
       " ('turn', 3),\n",
       " ('bet', 3),\n",
       " ('20', 3),\n",
       " ('hate', 3),\n",
       " ('Did', 3),\n",
       " ('far', 3),\n",
       " ('friend', 3),\n",
       " ('Then', 3),\n",
       " ('enemy', 3),\n",
       " ('Charlie', 3),\n",
       " ('inverted', 3),\n",
       " ('G', 3),\n",
       " ('push', 3),\n",
       " ('tells', 3),\n",
       " ('wrong', 3),\n",
       " ('classified', 3),\n",
       " ('went', 3),\n",
       " ('above', 3),\n",
       " ('Bullshit', 3),\n",
       " ('move', 3),\n",
       " ('engagement', 3),\n",
       " ('late', 3),\n",
       " ('again', 3),\n",
       " ('Why', 3),\n",
       " ('while', 3),\n",
       " ('Here', 3),\n",
       " ('mine', 3),\n",
       " ('am', 3),\n",
       " ('broke', 3),\n",
       " ('Sir', 3),\n",
       " ('Right', 3),\n",
       " ('blow', 3),\n",
       " ('makes', 3),\n",
       " ('guns', 3),\n",
       " ('aggressive', 3),\n",
       " ('chance', 3),\n",
       " ('maybe', 3),\n",
       " ('game', 3),\n",
       " ('Look', 3),\n",
       " ('MiG.', 3),\n",
       " ('always', 3),\n",
       " ('being', 3),\n",
       " ('told', 3),\n",
       " ('Hi', 3),\n",
       " ('choice', 3),\n",
       " ('away', 3),\n",
       " ('30', 3),\n",
       " ('say', 3),\n",
       " ('behind', 3),\n",
       " ('lead', 3),\n",
       " ('Wolfman', 3),\n",
       " ('wingman', 3),\n",
       " ('bogeys', 3),\n",
       " ('jetwash', 3),\n",
       " ('fault', 3),\n",
       " ('engine', 3),\n",
       " ('ready', 3),\n",
       " ('break', 3),\n",
       " ('Launch', 3),\n",
       " ('Firing', 3),\n",
       " ('Morning', 2),\n",
       " ('900', 2),\n",
       " ('closure', 2),\n",
       " ('clean', 2),\n",
       " ('3', 2),\n",
       " ('single', 2),\n",
       " ('range', 2),\n",
       " ('launch', 2),\n",
       " ('alert', 2),\n",
       " ('300', 2),\n",
       " ('locked', 2),\n",
       " ('bugging', 2),\n",
       " ('Mustang', 2),\n",
       " ('180', 2),\n",
       " ('bearing', 2),\n",
       " ('engaging', 2),\n",
       " ('fired', 2),\n",
       " ('perfect', 2),\n",
       " ('asshole', 2),\n",
       " ('Easy', 2),\n",
       " ('fun', 2),\n",
       " ('idea', 2),\n",
       " ('fuel', 2),\n",
       " ('heading', 2),\n",
       " ('trouble', 2),\n",
       " ('boys', 2),\n",
       " ('screwed', 2),\n",
       " ('stay', 2),\n",
       " ('walk', 2),\n",
       " ('park', 2),\n",
       " ('Call', 2),\n",
       " (\"I've\", 2),\n",
       " ('tight', 2),\n",
       " ('history', 2),\n",
       " ('high', 2),\n",
       " ('daughter', 2),\n",
       " ('Penny', 2),\n",
       " ('Benjamin', 2),\n",
       " ('lucky', 2),\n",
       " ('Navy', 2),\n",
       " ('screw', 2),\n",
       " ('butt', 2),\n",
       " ('send', 2),\n",
       " ('somebody', 2),\n",
       " ('weeks', 2),\n",
       " ('dog', 2),\n",
       " ('luck', 2),\n",
       " ('gentlemen', 2),\n",
       " ('During', 2),\n",
       " ('every', 2),\n",
       " ('ratio', 2),\n",
       " ('fell', 2),\n",
       " ('missiles', 2),\n",
       " ('teach', 2),\n",
       " ('end', 2),\n",
       " ('12', 2),\n",
       " ('please', 2),\n",
       " ('trophy', 2),\n",
       " ('fighter', 2),\n",
       " ('Commander', 2),\n",
       " ('Mike', 2),\n",
       " ('sign', 2),\n",
       " ('naval', 2),\n",
       " ('least', 2),\n",
       " ('each', 2),\n",
       " ('flown', 2),\n",
       " ('In', 2),\n",
       " ('case', 2),\n",
       " ('driver', 2),\n",
       " ('class', 2),\n",
       " ('option', 2),\n",
       " ('instructors', 2),\n",
       " ('Remember', 2),\n",
       " ('team', 2),\n",
       " ('school', 2),\n",
       " ('place', 2),\n",
       " ('Dismissed', 2),\n",
       " ('ladies', 2),\n",
       " ('room', 2),\n",
       " ('live', 2),\n",
       " ('life', 2),\n",
       " ('between', 2),\n",
       " ('Even', 2),\n",
       " ('happy', 2),\n",
       " ('Iceman', 2),\n",
       " ('mistakes', 2),\n",
       " ('stupid', 2),\n",
       " ('Tom', 2),\n",
       " ('Kazansky', 2),\n",
       " ('Still', 2),\n",
       " ('figure', 2),\n",
       " ('own', 2),\n",
       " ('slide', 2),\n",
       " ('spot', 2),\n",
       " ('Some', 2),\n",
       " ('career', 2),\n",
       " ('famous', 2),\n",
       " ('premises', 2),\n",
       " ('seem', 2),\n",
       " ('feeling', 2),\n",
       " ('she', 2),\n",
       " ('worry', 2),\n",
       " ('care', 2),\n",
       " ('new', 2),\n",
       " ('mother', 2),\n",
       " ('burned', 2),\n",
       " ('tomorrow', 2),\n",
       " ('Can', 2),\n",
       " ('ask', 2),\n",
       " ('personal', 2),\n",
       " ('Too', 2),\n",
       " ('comfortable', 2),\n",
       " ('save', 2),\n",
       " ('Really', 2),\n",
       " ('onewith', 2),\n",
       " ('yourself', 2),\n",
       " ('early', 2),\n",
       " ('morning', 2),\n",
       " ('trained', 2),\n",
       " ('most', 2),\n",
       " ('because', 2),\n",
       " ('yours', 2),\n",
       " ('Hello', 2),\n",
       " ('A-4s', 2),\n",
       " ('As', 2),\n",
       " ('flight', 2),\n",
       " ('negative', 2),\n",
       " ('clearance', 2),\n",
       " ('where', 2),\n",
       " ('six', 2),\n",
       " (\"ma'am\", 2),\n",
       " ('At', 2),\n",
       " ('foreign', 2),\n",
       " ('finger', 2),\n",
       " ('feet', 2),\n",
       " ('honey', 2),\n",
       " ('made', 2),\n",
       " ('security', 2),\n",
       " ('score', 2),\n",
       " ('Show', 2),\n",
       " ('run', 2),\n",
       " ('bring', 2),\n",
       " ('brakes', 2),\n",
       " ('burn', 2),\n",
       " ('l.', 2),\n",
       " ('ballistic', 2),\n",
       " ('return', 2),\n",
       " ('requesting', 2),\n",
       " ('pattern', 2),\n",
       " ('knew', 2),\n",
       " ('saw', 2),\n",
       " ('few', 2),\n",
       " ('took', 2),\n",
       " ('enjoyed', 2),\n",
       " ('Holy', 2),\n",
       " ('truck', 2),\n",
       " ('battle', 2),\n",
       " ('When', 2),\n",
       " ('straight', 2),\n",
       " ('huh', 2),\n",
       " ('tough', 2),\n",
       " ('well', 2),\n",
       " ('situation', 2),\n",
       " ('fast', 2),\n",
       " ('Slider', 2),\n",
       " ('stink', 2),\n",
       " ('Please', 2),\n",
       " ('apologies', 2),\n",
       " ('loved', 2),\n",
       " ('years', 2),\n",
       " ('listening', 2),\n",
       " ('died', 2),\n",
       " ('direct', 2),\n",
       " ('complicated', 2),\n",
       " ('shower', 2),\n",
       " ('understand', 2),\n",
       " ('job', 2),\n",
       " ('ass', 2),\n",
       " ('eight', 2),\n",
       " ('Freeze', 2),\n",
       " ('defensive', 2),\n",
       " ('bad', 2),\n",
       " ('last', 2),\n",
       " ('manoeuvre', 2),\n",
       " ('performance', 2),\n",
       " ('real', 2),\n",
       " ('through', 2),\n",
       " ('Multiple', 2),\n",
       " ('cover', 2),\n",
       " ('losing', 2),\n",
       " ('Keep', 2),\n",
       " ('switching', 2),\n",
       " ('Check', 2),\n",
       " ('worse', 2),\n",
       " ('embarrass', 2),\n",
       " ('world', 2),\n",
       " ('stud', 2),\n",
       " ('bed', 2),\n",
       " ('lose', 2),\n",
       " ('forever', 2),\n",
       " ('Fire', 2),\n",
       " ('Engine', 2),\n",
       " ('reach', 2),\n",
       " ('ejection', 2),\n",
       " ('handle', 2),\n",
       " (\"You've\", 2),\n",
       " ('responsibility', 2),\n",
       " ('without', 2),\n",
       " ('which', 2),\n",
       " ('His', 2),\n",
       " ('Engage', 2),\n",
       " ('goodbye', 2),\n",
       " ('Make', 2),\n",
       " ('rescue', 2),\n",
       " ('15', 2),\n",
       " ('Dead', 2),\n",
       " ('taking', 2),\n",
       " ('bringing', 2),\n",
       " ('Willard', 2),\n",
       " ('Simkin', 2),\n",
       " ('engaged', 2),\n",
       " ('tone', 2),\n",
       " ('Coming', 2),\n",
       " ('Strike', 2),\n",
       " ('[', 1),\n",
       " ('Scott', 1),\n",
       " ('Wells', 1),\n",
       " ('unknownaircraft', 1),\n",
       " ('Vector', 1),\n",
       " ('contact', 1),\n",
       " ('visual', 1),\n",
       " ('ID', 1),\n",
       " ('hook', 1),\n",
       " ('fry', 1),\n",
       " ('203', 1),\n",
       " ('inbound', 1),\n",
       " ('bogeyheading', 1),\n",
       " ('270', 1),\n",
       " ('angels', 1),\n",
       " ('0', 1),\n",
       " ('expecting', 1),\n",
       " ('visitors', 1),\n",
       " ('trailer?.Looks', 1),\n",
       " ('easy', 1),\n",
       " ('MiG-28s', 1),\n",
       " ('250', 1),\n",
       " ('sun', 1),\n",
       " (\"what'she\", 1),\n",
       " ('break150', 1),\n",
       " ('Airspeed', 1),\n",
       " ('scare', 1),\n",
       " ('himout', 1),\n",
       " ('Lock', 1),\n",
       " ('headed', 1),\n",
       " ('010', 1),\n",
       " ('permission', 1),\n",
       " ('until', 1),\n",
       " ('upon', 1),\n",
       " ('Bring', 1),\n",
       " ('Help', 1),\n",
       " ('shoot', 1),\n",
       " ('Greetings', 1),\n",
       " ('birdie', 1),\n",
       " ('Geez', 1),\n",
       " ('crack', 1),\n",
       " ('myself', 1),\n",
       " ('photographer', 1),\n",
       " ('bugged', 1),\n",
       " ('funfor', 1),\n",
       " ('running', 1),\n",
       " ('vapour', 1),\n",
       " ('ltback', 1),\n",
       " ('possible', 1),\n",
       " ('sucker', 1),\n",
       " ('order', 1),\n",
       " ('instructed', 1),\n",
       " ('immediately', 1),\n",
       " ('Have', 1),\n",
       " ('carrier', 1),\n",
       " ('onmy', 1),\n",
       " ('wing', 1),\n",
       " ('pull', 1),\n",
       " ('Pull', 1),\n",
       " ('Almost', 1),\n",
       " ('glide', 1),\n",
       " ('path', 1),\n",
       " ('Rogerball', 1),\n",
       " ('Increase', 1),\n",
       " ('power', 1),\n",
       " ('wife', 1),\n",
       " ('almost', 1),\n",
       " ('orphaned', 1),\n",
       " ('today', 1),\n",
       " ('scared', 1),\n",
       " ('holding', 1),\n",
       " ('edge', 1),\n",
       " ('incredibly', 1),\n",
       " ('brave', 1),\n",
       " ('landyour', 1),\n",
       " ('belongs', 1),\n",
       " ('taxpayers', 1),\n",
       " ('ego', 1),\n",
       " ('writing', 1),\n",
       " ('chequesyour', 1),\n",
       " ('body', 1),\n",
       " ('cash', 1),\n",
       " ('qualificationsas', 1),\n",
       " ('section', 1),\n",
       " ('leader', 1),\n",
       " ('times', 1),\n",
       " ('Put', 1),\n",
       " ('hack', 1),\n",
       " ('mewith', 1),\n",
       " ('passes', 1),\n",
       " ('towersand', 1),\n",
       " ('admiral', 1),\n",
       " ('bullshit', 1),\n",
       " (\"nameain't\", 1),\n",
       " ('itbetter', 1),\n",
       " ('cleaner', 1),\n",
       " ('serve', 1),\n",
       " ('country', 1),\n",
       " (\"I'd\", 1),\n",
       " ('bust', 1),\n",
       " ('Miramar', 1),\n",
       " ('believeit', 1),\n",
       " ('dream', 1),\n",
       " ('characters', 1),\n",
       " (\"you're\", 1),\n",
       " ('wasnumber', 1),\n",
       " ('turned', 1),\n",
       " ('wings', 1),\n",
       " ('remember', 1),\n",
       " (':', 1),\n",
       " ('flyinga', 1),\n",
       " ('cargo', 1),\n",
       " ('rubber', 1),\n",
       " ('Korea', 1),\n",
       " ('12of', 1),\n",
       " ('theirjets', 1),\n",
       " ('ours', 1),\n",
       " ('.Our', 1),\n",
       " ('dependant', 1),\n",
       " ('created', 1),\n",
       " ('ACM.Air', 1),\n",
       " ('Combat', 1),\n",
       " ('Manoeuvring', 1),\n",
       " ('Dogfighting', 1),\n",
       " ('gives', 1),\n",
       " ('By', 1),\n",
       " ('tease', 1),\n",
       " ('Blinds', 1),\n",
       " ('Our', 1),\n",
       " ('commanding', 1),\n",
       " ('officer', 1),\n",
       " ('veryfirst', 1),\n",
       " ('win', 1),\n",
       " ('finer', 1),\n",
       " ('pilotanywhere', 1),\n",
       " ('Metcalf', 1),\n",
       " ('top', 1),\n",
       " ('percentof', 1),\n",
       " ('aviators', 1),\n",
       " ('elite', 1),\n",
       " (\"We'll\", 1),\n",
       " ('missionsa', 1),\n",
       " ('attend', 1),\n",
       " ('classes', 1),\n",
       " ('evaluations', 1),\n",
       " ('sequence', 1),\n",
       " (\"you'regonna\", 1),\n",
       " ('meet', 1),\n",
       " ('different', 1),\n",
       " ('challenge', 1),\n",
       " ('F-14faster', 1),\n",
       " ('policy', 1),\n",
       " ('Elected', 1),\n",
       " ('officials', 1),\n",
       " ('civilians', 1),\n",
       " ('instruments', 1),\n",
       " ('mustalways', 1),\n",
       " ('war', 1),\n",
       " ('wondering', 1),\n",
       " ('wonderwho', 1),\n",
       " ('RIOfrom', 1),\n",
       " ('name', 1),\n",
       " ('hereand', 1),\n",
       " ('pretty', 1),\n",
       " ('arrogant', 1),\n",
       " ('considering', 1),\n",
       " ('company', 1),\n",
       " (\"we're\", 1),\n",
       " ('same', 1),\n",
       " ('Nice', 1),\n",
       " ('alternatesis', 1),\n",
       " ('O', 1),\n",
       " ('target', 1),\n",
       " ('rich', 1),\n",
       " ('environment', 1),\n",
       " ('legs', 1),\n",
       " ('laid', 1),\n",
       " (\"girlwho'd\", 1),\n",
       " ('talk', 1),\n",
       " ('dirty', 1),\n",
       " ('is?That', 1),\n",
       " ('flies', 1),\n",
       " ('ice', 1),\n",
       " ('cold', 1),\n",
       " ('Wears', 1),\n",
       " ('bored', 1),\n",
       " ('Whose', 1),\n",
       " ('kiss', 1),\n",
       " ('list', 1),\n",
       " ('distinguished', 1),\n",
       " ('myjohnson', 1),\n",
       " ('Mother', 1),\n",
       " ('tohear', 1),\n",
       " ('meant', 1),\n",
       " ('help?You', 1),\n",
       " ('First', 1),\n",
       " ('MiGand', 1),\n",
       " ('wait', 1),\n",
       " ('whole', 1),\n",
       " ('seea', 1),\n",
       " ('Lucky', 1),\n",
       " ('notorious', 1),\n",
       " ('See', 1),\n",
       " ('later', 1),\n",
       " ('abused', 1),\n",
       " ('children', 1),\n",
       " ('it?All', 1),\n",
       " ('dollars', 1),\n",
       " ('carnal', 1),\n",
       " ('knowledge', 1),\n",
       " ('lady', 1),\n",
       " ('fair', 1),\n",
       " ('loving', 1),\n",
       " ('miss', 1),\n",
       " ('Sit', 1),\n",
       " ('song', 1),\n",
       " ('approach', 1),\n",
       " ('Since', 1),\n",
       " ('Puberty', 1),\n",
       " ('Charlotte', 1),\n",
       " ('Blackwood', 1),\n",
       " ('aviator', 1),\n",
       " ('Actually', 1),\n",
       " ('Crashed', 1),\n",
       " ('arrived', 1),\n",
       " ('talking', 1),\n",
       " ('Perry', 1),\n",
       " ('question', 1),\n",
       " ('depends', 1),\n",
       " ('hold', 1),\n",
       " ('aboutyou', 1),\n",
       " ('making', 1),\n",
       " ('living', 1),\n",
       " ('singer', 1),\n",
       " ('beer', 1),\n",
       " ('put', 1),\n",
       " ('theseflames', 1),\n",
       " ('Real', 1),\n",
       " ('slick', 1),\n",
       " ('cruise', 1),\n",
       " ('sailor', 1),\n",
       " ('drop', 1),\n",
       " ('tileand', 1),\n",
       " ('actually', 1),\n",
       " ('counter', 1),\n",
       " ('very', 1),\n",
       " ('yeah', 1),\n",
       " ('came', 1),\n",
       " ('makinga', 1),\n",
       " ('mistake', 1),\n",
       " ('older', 1),\n",
       " ('bigger', 1),\n",
       " ('young', 1),\n",
       " ('magnificent', 1),\n",
       " ('also', 1),\n",
       " ('evaluatedby', 1),\n",
       " ('civilian', 1),\n",
       " ('specialists', 1),\n",
       " ('source', 1),\n",
       " ('informationon', 1),\n",
       " ('qualified', 1),\n",
       " ('signCharlie', 1),\n",
       " ('Ph.D.', 1),\n",
       " ('astrophysics', 1),\n",
       " ('Listen', 1),\n",
       " ('Pentagonlistens', 1),\n",
       " ('proficiency', 1),\n",
       " ('dealing', 1),\n",
       " ('F-5sand', 1),\n",
       " ('simulators', 1),\n",
       " ('F-5', 1),\n",
       " ('havethe', 1),\n",
       " ('thrust', 1),\n",
       " ('weight', 1),\n",
       " ('bleed', 1),\n",
       " ('energybelow', 1),\n",
       " ('However', 1),\n",
       " ('havea', 1),\n",
       " ('its', 1),\n",
       " ('tanks', 1),\n",
       " ('latest', 1),\n",
       " ('intelligence', 1),\n",
       " ('usthe', 1),\n",
       " ('data', 1),\n",
       " ('inaccurate', 1),\n",
       " ('MiG-28do', 1),\n",
       " ('dive', 1),\n",
       " ('Secret', 1),\n",
       " ('Pentagon', 1),\n",
       " ('sees', 1),\n",
       " ('itthat', 1),\n",
       " ('exactly', 1),\n",
       " ('started', 1),\n",
       " ('pulledthrough', 1),\n",
       " ('clouds', 1),\n",
       " ('directly', 1),\n",
       " ('Because', 1),\n",
       " ('divewith', 1),\n",
       " ('About', 1),\n",
       " ('metres', 1),\n",
       " ('11/2', 1),\n",
       " ('Polaroid', 1),\n",
       " ('nice', 1),\n",
       " ('picture', 1),\n",
       " ('Communicating', 1),\n",
       " ('Keeping', 1),\n",
       " ('relations', 1),\n",
       " ('giving', 1),\n",
       " ('bird', 1),\n",
       " ('10,000', 1),\n",
       " (\"There'll\", 1),\n",
       " ('Move', 1),\n",
       " ('meyou', 1),\n",
       " ('lnsulter', 1),\n",
       " ('Would', 1),\n",
       " ('difference', 1),\n",
       " ('Not', 1),\n",
       " ('hotshots', 1),\n",
       " ('eightweeks', 1),\n",
       " ('sure', 1),\n",
       " ('liketo', 1),\n",
       " ('sometime', 1),\n",
       " ('read', 1),\n",
       " (\"Maverick?.I'm\", 1),\n",
       " ('curious', 1),\n",
       " ('covering', 1),\n",
       " ('youwere', 1),\n",
       " ('showboating', 1),\n",
       " ('fine', 1),\n",
       " ('fiirsthop', 1),\n",
       " ('Thejets', 1),\n",
       " (\"you'reflying\", 1),\n",
       " ('smaller', 1),\n",
       " ('faster', 1),\n",
       " ('-justlike', 1),\n",
       " ('ofnow', 1),\n",
       " ('keeping', 1),\n",
       " ('Closing', 1),\n",
       " ('hide', 1),\n",
       " (\"I'll\", 1),\n",
       " ('money', 1),\n",
       " ('mountains', 1),\n",
       " ('vertical', 1),\n",
       " ('nail', 1),\n",
       " ('deckand', 1),\n",
       " ('base', 1),\n",
       " ('buzz', 1),\n",
       " ('tower', 1),\n",
       " ('balls', 1),\n",
       " ('everybody', 1),\n",
       " ('kicked', 1),\n",
       " ('go?\"He', 1),\n",
       " ('laughing', 1),\n",
       " ('radio', 1),\n",
       " ('dickhead', 1),\n",
       " ('Below', 1),\n",
       " ('nailed', 1),\n",
       " ('cowboys', 1),\n",
       " ('everyone', 1),\n",
       " ('Every', 1),\n",
       " ('unsafe', 1),\n",
       " ('Gooseget', 1),\n",
       " ('office', 1),\n",
       " ('brighter', 1),\n",
       " ('shut', 1),\n",
       " ('snot', 1),\n",
       " ('nosed', 1),\n",
       " ('jockeys', 1),\n",
       " ('byby', 1),\n",
       " ('400', 1),\n",
       " ('covers', 1),\n",
       " ('bys', 1),\n",
       " ('Follow', 1),\n",
       " ('was10,000', 1),\n",
       " ('followed', 1),\n",
       " ('Heatherlybelow', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(tokens)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "###  Tokenization\n",
    "\n",
    "The first preprocessing step is to tokenize each of the reviews into (lowercased) individual words, since the models will encode the reviews at the word level (rather than subword units like characters, for example). For this I'll use [spaCy](https://spacy.io/), which is a fast and extremely user-friendly library that performs various language processing tasks. Once you load a spaCy model for a particular language, you can provide any text as input to the model, e.g. encoder(text) and access its linguistic features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** \n",
    "1. Import spaCy\n",
    "2. Load English language\n",
    "3. Create a function ``text_totoken()`` that takes the text sequences in argument and returns the tokens\n",
    "4. Create a column in the dataframe ``train_reviews`` and save your tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'pos__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-0202792da05e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#token = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-0202792da05e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#token = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'pos__'"
     ]
    }
   ],
   "source": [
    "'''Split texts into lists of words (tokens)'''\n",
    "### ENTER YOUR CODE HERE ####\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "[print(token.pos__) for token in doc]\n",
    "\n",
    "#token = []\n",
    "\n",
    "#for i in doc:\n",
    "#    i.text\n",
    "#token = tuple(token)\n",
    "    #print(token.lemma_)\n",
    "    #print(token.pos_)\n",
    "    #print(token.tag_)\n",
    "    #print(token.dep_)\n",
    "    #print(token.shape_)\n",
    "    #print(token.is_alpha)\n",
    "    #print(token.is_stop)\n",
    "    \n",
    "### END \n",
    "#train_reviews[['Review','Tokenized_Review']][:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{23656}{23827}During Vietnam, that ratio fell to 3-1 .|Our pilots were dependant on missiles.\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_py_tokens', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_disk', 'get_extension', 'get_lca_matrix', 'has_extension', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'merge', 'noun_chunks', 'noun_chunks_iterator', 'print_tree', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_extension', 'similarity', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Morning, Scott\n"
     ]
    }
   ],
   "source": [
    "noun_chunks = list(doc.noun_chunks)\n",
    "print(noun_chunks[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick and Goose.', '- Talk to me, Goose.\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(sentences[5].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Morning 1 9 WORK_OF_ART\n",
      "Scott 11 16 PERSON\n",
      "Morning 17 24 TIME\n",
      "Wells 26 31 ORG\n",
      "'Ghost Rider 35 47 WORK_OF_ART\n",
      "090 84 87 CARDINAL\n",
      "Cougar 120 126 PERSON\n",
      "Merlin 128 134 PERSON\n",
      "Maverick 136 144 GPE\n",
      "Goose 149 154 GPE\n",
      "Maverick 166 174 PERSON\n",
      "Goose 179 184 PERSON\n",
      "'- Talk to me, Goose 188 208 WORK_OF_ART\n",
      "Roger 210 215 PERSON\n",
      "'- Cougar 250 259 PERSON\n",
      "Roger 276 281 PERSON\n",
      "\"- Merlin 285 294 PERSON\n",
      "'Ghost Rider 408 420 PERSON\n",
      "203 422 425 CARDINAL\n",
      "270 460 463 CARDINAL\n",
      "10 miles 467 475 QUANTITY\n",
      "10 513 515 CARDINAL\n",
      "3 523 524 CARDINAL\n",
      "\"- Merlin 578 587 PERSON\n",
      "Roger 611 616 PERSON\n",
      "Mav 785 788 PERSON\n",
      "two 843 846 CARDINAL\n",
      "250 miles 899 908 QUANTITY\n",
      "Cougar 944 950 PERSON\n",
      "MiG 963 966 PERSON\n",
      "MiG 987 990 PERSON\n",
      "Shit 1022 1026 PERSON\n",
      "Merlin 1057 1063 PERSON\n",
      "five 1210 1214 CARDINAL\n",
      "300 1239 1242 CARDINAL\n",
      "Mustang 1421 1428 PERSON\n",
      "MiG 1430 1433 PERSON\n",
      "180 miles 1482 1491 QUANTITY\n",
      "010 1504 1507 CARDINAL\n",
      "Mustang 1581 1588 PERSON\n",
      "MiG 1705 1708 PERSON\n",
      "Maverick 1780 1788 PERSON\n",
      "Cougar 1824 1830 PERSON\n",
      "Geez 2006 2010 PERSON\n",
      "Cougar 2076 2082 PERSON\n",
      "MiG 2103 2106 PERSON\n",
      "Cougar 2130 2136 PERSON\n",
      "one day 2162 2169 DATE\n",
      "Cougar 2297 2303 PERSON\n",
      "Cougar 2341 2347 PERSON\n",
      "Cougar 2439 2445 PERSON\n",
      "'- Maverick 2523 2534 WORK_OF_ART\n",
      "Roger 2551 2556 PERSON\n",
      "Maverick 2558 2566 PERSON\n",
      "Cougar 2586 2592 PERSON\n",
      "Cougar 2671 2677 PERSON\n",
      "\"- Goddamn it! 2681 2695 WORK_OF_ART\n",
      "Cougar 2745 2751 PERSON\n",
      "MiG 2884 2887 PERSON\n",
      "Cougar 2961 2967 PERSON\n",
      "Cougar 3029 3035 PERSON\n",
      "Cougar 3149 3155 PERSON\n",
      "Rogerball 3233 3242 PERSON\n",
      "today 3364 3369 DATE\n",
      "Cougar 3521 3527 PERSON\n",
      "Maverick 3541 3549 GPE\n",
      "\"- Maverick 3553 3564 PERSON\n",
      "Goose 3566 3571 GPE\n",
      "Shit 3632 3636 LOC\n",
      "Maverick 3641 3649 PERSON\n",
      "three 3871 3876 CARDINAL\n",
      "five 3958 3962 CARDINAL\n",
      "one 3985 3988 CARDINAL\n",
      "Navy 4130 4134 ORG\n",
      "Miramar 4407 4414 PERSON\n",
      "five weeks 4586 4596 DATE\n",
      "two 4654 4657 CARDINAL\n",
      "one 4749 4752 CARDINAL\n",
      "'Tell me about 4863 4877 WORK_OF_ART\n",
      "MiG 4882 4885 ORG\n",
      "Korea 4966 4971 GPE\n",
      "12of 4986 4990 ORDINAL\n",
      "one 5011 5014 CARDINAL\n",
      "Vietnam 5034 5041 GPE\n",
      "3 5062 5063 CARDINAL\n",
      "'Top Gun was created 5108 5128 WORK_OF_ART\n",
      "the end of 5212 5222 DATE\n",
      "Vietnam 5223 5230 GPE\n",
      "12 5239 5241 CARDINAL\n",
      "the Top Gun 5338 5349 WORK_OF_ART\n",
      "Mike Metcalf 5417 5429 PERSON\n",
      "at least two 5589 5601 CARDINAL\n",
      "you'regonna 5683 5694 PRODUCT\n",
      "F-14faster 5753 5763 PRODUCT\n",
      "RIOfrom 6093 6100 ORG\n",
      "second 6474 6480 ORDINAL\n",
      "Jesus 6580 6585 PERSON\n",
      "two 6631 6634 CARDINAL\n",
      "Goose 6642 6647 GPE\n",
      "Iceman 6877 6883 PERSON\n",
      "Wears 6923 6928 NORP\n",
      "lceman 7155 7161 PERSON\n",
      "Mother Goose 7168 7180 PERSON\n",
      "Tom 7201 7204 PERSON\n",
      "Pete Mitchell 7214 7227 PERSON\n",
      "Tom Kazansky 7228 7240 PERSON\n",
      "Cougar 7292 7298 PERSON\n",
      "First 7567 7572 ORDINAL\n",
      "MiGand 7577 7583 PERSON\n",
      "Cougar 7604 7610 ORG\n",
      "Cougar 7643 7649 ORG\n",
      "MiG 7701 7704 ORG\n",
      "20 dollars 7918 7928 MONEY\n",
      "'- Sit 8322 8328 PERSON\n",
      "Puberty 8457 8464 PERSON\n",
      "Charlotte Blackwood 8476 8495 ORG\n",
      "Maverick 8501 8509 PERSON\n",
      "first 8711 8716 ORDINAL\n",
      "second 8730 8736 ORDINAL\n",
      "tomorrow 8755 8763 DATE\n",
      "Maverick 8803 8811 PERSON\n",
      "second 8908 8914 ORDINAL\n",
      "Mav 9150 9153 PERSON\n",
      "early in the morning 9550 9570 TIME\n",
      "Ph.D. 9806 9811 WORK_OF_ART\n",
      "Pentagonlistens 9859 9874 PERSON\n",
      "Charlie 9920 9927 PERSON\n",
      "F-5sand A-4s 9963 9975 PRODUCT\n",
      "MiG 9983 9986 PERSON\n",
      "F-5 10019 10022 PRODUCT\n",
      "MiG-28 10069 10075 PRODUCT\n",
      "300 10113 10116 CARDINAL\n",
      "MiG-28 10132 10138 PRODUCT\n",
      "MiG-28 10156 10162 PRODUCT\n",
      "MiG 10387 10390 ORG\n",
      "MiG-28 10452 10458 PRODUCT\n",
      "Goose 10469 10474 PERSON\n",
      "four 10511 10515 CARDINAL\n",
      "Pentagon 10669 10677 ORG\n",
      "six 10810 10813 CARDINAL\n",
      "four 11026 11030 CARDINAL\n",
      "Polaroid 11137 11145 ORG\n",
      "10,000 feet 11516 11527 QUANTITY\n",
      "MiG 11684 11687 FAC\n",
      "20 11773 11775 CARDINAL\n",
      "MiG 11882 11885 ORG\n",
      "Cougar 11998 12004 PERSON\n",
      "MiG 12040 12043 PERSON\n",
      "Cougar 12048 12054 PERSON\n",
      "Thejets you'reflying 12103 12123 ORG\n",
      "MiGs 12181 12185 LOC\n",
      "Goose 12234 12239 PERSON\n",
      "Goose 12377 12382 PERSON\n",
      "Jester 12480 12486 PERSON\n",
      "Mav 12587 12590 PERSON\n",
      "Jesus Christ 12747 12759 PERSON\n",
      "Jester 12797 12803 PERSON\n",
      "Jester 12955 12961 PERSON\n",
      "Jester 13002 13008 PERSON\n",
      "Jester 13017 13023 PERSON\n",
      "Ghost Rider 13115 13126 PERSON\n",
      "Mav 13188 13191 PERSON\n",
      "Jester 13314 13320 PERSON\n",
      "Hollywood 13410 13419 GPE\n",
      "Jester 13614 13620 PERSON\n",
      "Kazansky 13748 13756 PERSON\n",
      "lceman 13878 13884 PERSON\n",
      "Maverick 13908 13916 PERSON\n",
      "Gooseget 13927 13935 PERSON\n",
      "Viper 13953 13958 PERSON\n",
      "second 14001 14007 ORDINAL\n",
      "Two 14073 14076 CARDINAL\n",
      "400 14122 14125 CARDINAL\n",
      "a first day 14281 14292 DATE\n",
      "Heatherlybelow 14393 14407 PERSON\n",
      "a few seconds 14563 14576 TIME\n",
      "'Top Gun rules of engagement existfor 14746 14783 WORK_OF_ART\n",
      "Mav 14955 14958 PERSON\n",
      "Maverick 15182 15190 PERSON\n",
      "Top Gun 15398 15405 ORG\n",
      "Mav 15484 15487 PERSON\n",
      "Academy 15662 15669 ORG\n",
      "Duke Mitchell's 15697 15712 PERSON\n",
      "Mav 16515 16518 PERSON\n",
      "Slider 16524 16530 PERSON\n",
      "Mav 16574 16577 PERSON\n",
      "one 16584 16587 CARDINAL\n",
      "one 16661 16664 CARDINAL\n",
      "he\\ 16960 16963 ORG\n",
      "himwhy you\\'ve 17009 17023 PERSON\n",
      "MiG-28 17113 17119 PRODUCT\n",
      "this in years 17457 17470 DATE\n",
      "just listening for hours 17613 17637 TIME\n",
      "an F-4,November 5th, 1965 17872 17897 DATE\n",
      "second 18122 18128 ORDINAL\n",
      "eight 18825 18830 CARDINAL\n",
      "one 18886 18889 CARDINAL\n",
      "F-14 19070 19074 PRODUCT\n",
      "aircraftthan 19161 19173 GPE\n",
      "Charlie 19198 19205 PERSON\n",
      "another three seconds 19254 19275 TIME\n",
      "five 19351 19355 CARDINAL\n",
      "'- Charlie 19413 19423 PERSON\n",
      "one 19434 19437 CARDINAL\n",
      "MiG 19501 19504 WORK_OF_ART\n",
      "MiG 19548 19551 ORG\n",
      "$30 million 19697 19708 MONEY\n",
      "MiG 19748 19751 ORG\n",
      "Maverick 19779 19787 PERSON\n",
      "Jesus 20151 20156 PERSON\n",
      "first 20221 20226 ORDINAL\n",
      "Maverick 20487 20495 PERSON\n",
      "lce 20509 20512 PERSON\n",
      "19 20618 20620 CARDINAL\n",
      "lceman 20744 20750 PERSON\n",
      "Second 20755 20761 ORDINAL\n",
      "Maverick 20763 20771 GPE\n",
      "two 20773 20776 CARDINAL\n",
      "Three miles 20811 20822 QUANTITY\n",
      "10 o'clock 20824 20834 TIME\n",
      "Two 20851 20854 CARDINAL\n",
      "A-4s 20855 20859 PRODUCT\n",
      "10 o'clock 20865 20875 TIME\n",
      "1 10 20941 20945 DATE\n",
      "it\\ 21018 21021 ORG\n",
      "Maverick 21024 21032 GPE\n",
      "Goose 21037 21042 GPE\n",
      "Roger 21089 21094 PERSON\n",
      "Hollywood 21126 21135 GPE\n",
      "Jester 21177 21183 PERSON\n",
      "Jester 21226 21232 PERSON\n",
      "Hollywood 21284 21293 GPE\n",
      "Three o'clock 21315 21328 TIME\n",
      "Hollywood 21340 21349 GPE\n",
      "Hollywood 21384 21393 GPE\n",
      "Viper 21435 21440 PERSON\n",
      "Hollywood 21480 21489 GPE\n",
      "Damn 21832 21836 PERSON\n",
      "Shit 21981 21985 PERSON\n",
      "Jester 21995 22001 PERSON\n",
      "Bingo 22006 22011 PERSON\n",
      "Maverick 22013 22021 GPE\n",
      "The Defence Department 22058 22080 ORG\n",
      "- 30 seconds 22196 22208 TIME\n",
      "Wolfman 22210 22217 PERSON\n",
      "Maverick 22358 22366 PERSON\n",
      "lceman beforehe 22579 22594 PERSON\n",
      "\"- Hell no. 22781 22792 WORK_OF_ART\n",
      "Penny Benjamin 22898 22912 PERSON\n",
      "Carole 23056 23062 PERSON\n",
      "Goose 23101 23106 PERSON\n",
      "Pete 23218 23222 PERSON\n",
      "One 23242 23245 CARDINAL\n",
      "tonight 23324 23331 TIME\n",
      "100%%% 23395 23401 PERSON\n",
      "'Hey, Goose 23433 23444 WORK_OF_ART\n",
      "Maverick 23572 23580 PERSON\n",
      "31.Two weeks 23652 23664 DATE\n",
      "'The Top Gun trophyis 23682 23703 WORK_OF_ART\n",
      "Maverick 23797 23805 GPE\n",
      "Two miles 23862 23871 QUANTITY\n",
      "two o'clock 23935 23946 TIME\n",
      "Jesus Christ 24011 24023 PERSON\n",
      "lce 24025 24028 GPE\n",
      "20 seconds 24195 24205 TIME\n",
      "Mav 24272 24275 PERSON\n",
      "10 more seconds 24321 24336 TIME\n",
      "Five more seconds 24399 24416 TIME\n",
      "one 24523 24526 CARDINAL\n",
      "two 24545 24548 CARDINAL\n",
      "8,000 24727 24732 CARDINAL\n",
      "7,000 24737 24742 CARDINAL\n",
      "6,000 24759 24764 CARDINAL\n",
      "'- Eject 24881 24889 WORK_OF_ART\n",
      "Goose 24915 24920 PERSON\n",
      "'- Goose 25039 25047 WORK_OF_ART\n",
      "RIO 25134 25137 ORG\n",
      "Vietnam 25179 25186 GPE\n",
      "8 25195 25196 CARDINAL\n",
      "18 25200 25202 CARDINAL\n",
      "10 25213 25215 CARDINAL\n",
      "first 25228 25233 ORDINAL\n",
      "one 25234 25237 CARDINAL\n",
      "Maverick 25599 25607 GPE\n",
      "yaw rateto 25852 25862 PERSON\n",
      "Lieutenant Mitchellcould 25929 25953 PERSON\n",
      "'The Board of lnquiry 25982 26003 WORK_OF_ART\n",
      "29 July 26053 26060 DATE\n",
      "two o'clock 26270 26281 TIME\n",
      "Maverick 26334 26342 GPE\n",
      "a few days 26601 26611 DATE\n",
      "Mitchell 26674 26682 PERSON\n",
      "Goose 26703 26708 PERSON\n",
      "Wolfman 26760 26767 PERSON\n",
      "Washington 26953 26963 GPE\n",
      "going?You 27092 27101 ORG\n",
      "Navy 27261 27265 ORG\n",
      "first 27373 27378 ORDINAL\n",
      "'Look at you 27416 27428 WORK_OF_ART\n",
      "you'regoing Mach 2 27466 27484 PERSON\n",
      "Pete Mitchell 27896 27909 PERSON\n",
      "Maverick 27918 27926 GPE\n",
      "Jamie 27991 27996 PERSON\n",
      "Mike 28060 28064 PERSON\n",
      "VF-51 28113 28118 PRODUCT\n",
      "Oriskany 28125 28133 GPE\n",
      "F-4 28539 28542 PRODUCT\n",
      "three 28601 28606 CARDINAL\n",
      "You've 28816 28822 PERSON\n",
      "tomorrow 28864 28872 DATE\n",
      "Goose 29048 29053 FAC\n",
      "Sunday 29336 29342 DATE\n",
      "Maverick 29426 29434 GPE\n",
      "Mav 29608 29611 PERSON\n",
      "Hollywood 29774 29783 GPE\n",
      "Wolfman 29785 29792 PERSON\n",
      "'- Maverick 29796 29807 WORK_OF_ART\n",
      "Maverick 29817 29825 PERSON\n",
      "RIOwhen 29843 29850 ORG\n",
      "The SS Layton 29934 29947 ORG\n",
      "MiGs 30099 30103 FAC\n",
      "Exocet 30177 30183 PRODUCT\n",
      "100 miles 30224 30233 QUANTITY\n",
      "us 30312 30314 GPE\n",
      "Hollywood 30329 30338 GPE\n",
      "two 30347 30350 CARDINAL\n",
      "Maverick 30355 30363 PERSON\n",
      "Merlinon 30387 30395 PERSON\n",
      "five 30402 30406 CARDINAL\n",
      "Maverick 30482 30490 GPE\n",
      "15 miles 30595 30603 QUANTITY\n",
      "One 30615 30618 CARDINAL\n",
      "090 30620 30623 CARDINAL\n",
      "500 30662 30665 CARDINAL\n",
      "Roger 30676 30681 PERSON\n",
      "Dead 30705 30709 PERSON\n",
      "15 miles 30717 30725 QUANTITY\n",
      "600 30727 30730 CARDINAL\n",
      "15,000 30752 30758 CARDINAL\n",
      "10 miles 30768 30776 QUANTITY\n",
      "800 30781 30784 CARDINAL\n",
      "eight miles 30808 30819 QUANTITY\n",
      "Roger 30933 30938 PERSON\n",
      "Maverick 30979 30987 PERSON\n",
      "five 31012 31016 CARDINAL\n",
      "Mybogey 31021 31028 PERSON\n",
      "090 31104 31107 CARDINAL\n",
      "Four 31197 31201 CARDINAL\n",
      "four 31229 31233 CARDINAL\n",
      "five 31264 31268 CARDINAL\n",
      "five 31278 31282 CARDINAL\n",
      "Hollywood 31354 31363 GPE\n",
      "Wood 31437 31441 ORG\n",
      "Wood 31454 31458 ORG\n",
      "One 31481 31484 CARDINAL\n",
      "Wood 31486 31490 ORG\n",
      "Hollywood 31513 31522 GPE\n",
      "Launch Maverick 31550 31565 PERSON\n",
      "Hollywood andWolfman 31587 31607 PERSON\n",
      "\"- Iceman 31641 31650 PERSON\n",
      "090 31674 31677 CARDINAL\n",
      "180 miles 31681 31690 QUANTITY\n",
      "Three 31721 31726 CARDINAL\n",
      "MiGs 31727 31731 NORP\n",
      "four 31815 31819 CARDINAL\n",
      "six 31843 31846 CARDINAL\n",
      "'- Launch the alertfiighters! 31927 31956 WORK_OF_ART\n",
      "160 miles 31957 31966 QUANTITY\n",
      "Ready Willard 31996 32009 PERSON\n",
      "30 seconds 32044 32054 TIME\n",
      "five 32073 32077 CARDINAL\n",
      "\"- Willard and Simkin 32099 32120 WORK_OF_ART\n",
      "10 32174 32176 CARDINAL\n",
      "two minutes 32198 32209 TIME\n",
      "One MiG 12 o'clock 32214 32232 TIME\n",
      "'- Break right! 32260 32275 WORK_OF_ART\n",
      "Shit 32276 32280 PERSON\n",
      "'- Maverick 32300 32311 PERSON\n",
      "Hang 32336 32340 PERSON\n",
      "Jesus Christ 32354 32366 PERSON\n",
      "MiG 32415 32418 ORG\n",
      "Maverick 32626 32634 GPE\n",
      "\"- Ice 32723 32729 PERSON\n",
      "Goose 32774 32779 PERSON\n",
      "Maverick 32797 32805 PERSON\n",
      "MiG 32874 32877 ORG\n",
      "155 miles 32935 32944 QUANTITY\n",
      "Two 32951 32954 CARDINAL\n",
      "MiGs 32955 32959 NORP\n",
      "MiG 33047 33050 PRODUCT\n",
      "lce 33122 33125 PERSON\n",
      "two 33132 33135 CARDINAL\n",
      "One 33157 33160 CARDINAL\n",
      "MiG 33161 33164 ORG\n",
      "MiG 33201 33204 PRODUCT\n",
      "Roger 33516 33521 PERSON\n",
      "MiG 33618 33621 PRODUCT\n",
      "Shoot 33843 33848 WORK_OF_ART\n",
      "three 33932 33937 CARDINAL\n",
      "Three 33960 33965 CARDINAL\n",
      "two 33970 33973 QUANTITY\n",
      "three 34097 34102 CARDINAL\n",
      "one 34152 34155 CARDINAL\n",
      "'1 10 miles 34172 34183 QUANTITY\n",
      "four 34411 34415 CARDINAL\n",
      "Three 34435 34440 CARDINAL\n",
      "Maverick 34486 34494 GPE\n",
      "'Goddamn that guy 34591 34608 WORK_OF_ART\n",
      "Bullshit 34687 34695 WORK_OF_ART\n",
      "Maverick 34717 34725 PERSON\n",
      "English 34764 34771 LANGUAGE\n",
      "Pete Mitchell 35023 35036 PERSON\n",
      "first 35150 35155 ORDINAL\n",
      "second 35194 35200 ORDINAL\n",
      "'THE END' 35254 35263 WORK_OF_ART\n",
      "Skimaniac 35279 35288 PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-25385b902201>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking at buying U.K. startup for 1$"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Each tokenized word in the reviews is added to the lexicon, and then each word is mapped to a numerical index that can be read by the model. Since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times, and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. When assigning the indices, the number 1 will represent unknown words. The number 0 will represent \"empty\" word slots, which is explained below. Therefore \"real\" words will have indices of 2 or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** \n",
    "1. Create a function ``make_lexicon()``\n",
    "2. Create a lexicon variable and use your function ``make_lexicon()``\n",
    "3. Save your lexicon with pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Count tokens (words) in texts and add them to the lexicon'''\n",
    "### ENTER YOUR CODE HERE \n",
    "#### Create a function make_lexicon()\n",
    "import pickle\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1, use_padding=False):\n",
    "    # First, count how often each word appears in the text.\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pass\n",
    "### Create a lexicon variable and use your function make_lexicon() \n",
    "\n",
    "\n",
    "#### Save your lexicon with pickle\n",
    "\n",
    "    \n",
    "#### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  From strings to numbers\n",
    "\n",
    "Once the lexicon is built, we can use it to transform each review from a list of string tokens into a list of numerical indices.\n",
    "\n",
    "**Exercise :**\n",
    "1. Create tokens_to_idx function that transforms the lexicon into an index.\n",
    "2. Create a new ``Review_Idxs`` column in train_reviews and use your function ``tokens_to_idx`` on train_reviews['Tokenized_Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert each text from a list of tokens to a list of numbers (indices)'''\n",
    "### ENTER YOUR CODE HERE (4-5 lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### END                                   \n",
    "train_reviews[['Tokenized_Review', 'Review_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building a Multilayer Perceptron\n",
    "\n",
    "Before I show how to build an RNN for this task, I'll demonstrate an even simpler model, a multilayer perceptron (MLP). Unlike an RNN, an MLP model is not a sequence model - it represents data as a flat matrix of features rather than a time-ordered sequence of features. For language data, this generally means that the word order of a sequence will not be explicitly encoded into a model. The importance of word order varies for different NLP tasks; in some cases, order-sensitive approaches do not necessarily perform better.\n",
    "\n",
    "### Numerical lists to bag-of-words vectors\n",
    "\n",
    "The simplest and most common representation of a text in NLP is as a bag-of-words vector. A bag-of-words vector encodes a sequence as an array with a dimension for each word in the lexicon. The value for each dimension is the number of times the word corresponding to that dimension appears in the text. Thus a dataset of text sequences is encoded as a matrix where each row represents a sequence and each column represents a word whose value is the frequency of that word in the sequence (it is also common to apply some weighting function to these values such as [tf-idf](https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html), but here we'll just use counts).\n",
    "\n",
    "You may notice that the first dimension of the matrix has a count of 0 for all reviews, since there are no words represented by 0. This dimension is only relevant for the RNN model, where 0 will be used in a special way to indicate null words. This is explained fully below. For the MLP model, including this dimension won't make a difference because the model will learn to ignore it in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Encode reviews as bag-of-words vectors'''\n",
    "\n",
    "import numpy \n",
    "\n",
    "def idx_seqs_to_bows(idx_seqs, matrix_length):\n",
    "    bow_seqs = numpy.array([numpy.bincount(numpy.array(idx_seq), minlength=matrix_length) \n",
    "                            for idx_seq in idx_seqs])\n",
    "    return bow_seqs\n",
    "    \n",
    "bow_train_reviews = idx_seqs_to_bows(train_reviews['Review_Idxs'], \n",
    "                                     matrix_length=len(lexicon) + 1) #add one to length for padding)\n",
    "print(\"TRAIN INPUT:\\n\", bow_train_reviews)\n",
    "print(\"SHAPE:\", bow_train_reviews.shape, \"\\n\")\n",
    "\n",
    "#Show an example mapping string words to counts\n",
    "lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "lexicon_lookup[0] = \"\"\n",
    "pandas.DataFrame([(lexicon_lookup[idx], count) for idx, count in enumerate(bow_train_reviews[0])], \n",
    "                 columns=['Word', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Keras Model\n",
    "\n",
    "To assemble the model, we'll use Keras' [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to use Keras to assemble models (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). A model consists of a series of layers. As shown in the code below, we initialize instances for each layer. Each layer can be called with another layer as input, e.g. Dense()(input_layer). A model instance is initialized with the Model() object, which defines the initial input and final output layers for that model. Before the model can be trained, the compile() function must be called with the loss function and optimization algorithm specified (see below).\n",
    "\n",
    "###  Layers\n",
    "\n",
    "We'll build an MLP with four layers:\n",
    "\n",
    "**1. Input**: The input layer takes in the matrix of sequence vectors.\n",
    "\n",
    "**2. Dense (sigmoid activation)**: A hidden [layer](https://keras.io/layers/core/#dense), which is what defines the model as a multilayer perceptron. This layer transforms the input matrix by applying a nonlinear transformation function (here, the sigmoid function). Intuitively, this layer can be thought of as computing a \"feature representation\" of the input words matrix. \n",
    "\n",
    "**3. Dense (linear activation)**: An output layer that predicts the rating for the review based on its hidden representation given by the previous layer. This output is continuous (i.e. ranging from 1-10) rather than categorical, which means it has linear activation rather than nonlinear like the hidden layer (by default, activation='linear' for the Dense layer in Keras). The model gets feedback during training about what the actual ratings for the reviews should be.\n",
    "\n",
    "The term \"layer\" is just an abstraction, when really all these layers are just matrices. The \"weights\" that connect the layers are also matrices. The process of training a neural network is a series of matrix multiplications. The weight matrices are the values that are adjusted during training in order for the model to learn to predict ratings. \n",
    "\n",
    "###  Parameters\n",
    "\n",
    "Our function for creating the model takes two parameters:\n",
    "\n",
    "**n_input_nodes**: In the case of reviews encoded as bag-of-words vectors, this is the number of unique words in the lexicon, plus one to account for the padding represented by 0 values (which are only relevant for the RNN model, but this dimension can be included here without any cost to the model).\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layers. This can be freely chosen; here, it is set to 500.\n",
    "\n",
    "###  Procedure\n",
    "\n",
    "The output of the model is a single continuous value (the predicted rating), making this a regression rather than a classification model. There is only one dimension in the output layer, which contains the predicted rating. All neural networks learn by updating the parameters (weights) to optimize an objective (loss) function. For this model, the objective is to minimize the mean squared error between the predicted ratings and the actual ratings for the training reviews, thus bringing the predicted ratings closer to the real ratings. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models. The specific loss function and optimization method you use is specified when compiling the model with the compile() function.\n",
    "\n",
    "### Exercise \n",
    "\n",
    "1. Create a ``create_mlp_model`` function. This finction will create an MLP model\n",
    "2. This model must have the following layers :\n",
    "    1. **Input**: The input layer takes in the matrix of sequence vectors\n",
    "    2. **Dense (sigmoid activation)**: A hidden [layer](https://keras.io/layers/core/#dense), which is what defines the model as a multilayer perceptron.\n",
    "    3. **Dense (linear activation)**: An output layer that predicts the rating for the review based on its hidden representation given by the previous layer.\n",
    "    4. Dont forget compile your model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the Multilayer Perceptron model'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "def create_mlp_model(n_input_nodes, n_hidden_nodes):\n",
    "    ### ENTER YOUR CODE HERE \n",
    "    \n",
    "    # Layer 1 -  Technically the shape of this layer is (batch_size, len(n_input_nodes).\n",
    "    # The batch size is implicitly included in the shape of the input, so it does not need to \n",
    "    # be specified as a dimension of the input.\n",
    "    \n",
    "    \n",
    "    #Shape = (batch_size, n_input_nodes)\n",
    "    \n",
    "    \n",
    "    # layer 2 : DENSE, Sigmoid activation\n",
    "\n",
    "    #Output shape = (batch_size, n_hidden_nodes)\n",
    "    \n",
    "    \n",
    "    #Layer 3 : Dense \n",
    "\n",
    "    #Output shape = (batch_size, 1)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    \n",
    "    # END \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_bow_model = create_mlp_model(n_input_nodes=len(lexicon) + 1, n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training\n",
    "\n",
    "Now we can train an MLP model on the training reviews encoded as a bag-of-words matrix. Keras will apply batch training by default, even though we didn't specify the batch size when creating the model. If a batch size isn't given, Keras will use its default (32). The training function also indicates the number of times to iterate through the training data (epochs). Keras reports the mean squared error loss after each epoch - if the model is learning correctly, it should progressively decrease.\n",
    "\n",
    "**Erxecise :** Fit your model and save it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train the MLP model with bag-of-words representation'''\n",
    "### ENTER YOUR CODE ( 2 Lines)\n",
    "\n",
    "\n",
    "#save model\n",
    "\n",
    "\n",
    "#### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting ratings for reviews\n",
    "\n",
    "Once the model is trained, we can use it to predict ratings for the reviews in the test set. To demonstrate this, I'll load a saved model previously trained (for 25 epochs) on all 25,000 reviews in the training set. I'll apply this model to an example test set of 100 reviews (again, this is a tiny subset of the 25,000 reviews in the full test set provided at the above link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load saved model'''\n",
    "\n",
    "# Load lexicon\n",
    "with open('pretrained_model/mlp_bow/lexicon.pkl', 'rb') as f:\n",
    "    mlp_bow_lexicon = pickle.load(f)\n",
    "\n",
    "# Load MLP BOW model\n",
    "from keras.models import load_model\n",
    "mlp_bow_model = load_model('pretrained_model/mlp_bow/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Create test_reviews variable and apply exactly same processing on test dataset. ``(dataset/example_test_imdb_reviews.csv)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load the test dataset, tokenize, and transform to numerical indices'''\n",
    "### ENTER YOUR CODE HERE (+-3 lines)\n",
    "test_reviews = ...\n",
    "test_reviews['Tokenized_Review'] = ...\n",
    "test_reviews['Review_Idxs'] = ...\n",
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transform test reviews to a bag-of-words matrix'''\n",
    "\n",
    "bow_test_reviews = idx_seqs_to_bows(test_reviews['Review_Idxs'], \n",
    "                                    matrix_length=len(mlp_bow_lexicon) + 1) #add one to length for padding)\n",
    "\n",
    "print(\"TEST INPUT:\\n\", bow_test_reviews)\n",
    "print(\"SHAPE:\", bow_test_reviews.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can call the predict() function on the test reviews to get the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show predicted ratings for test reviews alongside actual ratings'''\n",
    "\n",
    "#Since ratings are integers, need to round predicted rating to nearest integer\n",
    "test_reviews['MLP_BOW_Pred_Rating'] = numpy.round(mlp_bow_model.predict(bow_test_reviews)[:,0]).astype(int)\n",
    "test_reviews[['Review', 'Rating', 'MLP_BOW_Pred_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluation\n",
    "\n",
    "A common evaluation for regression models like this one is $R^2$, called the the coefficient of determination. This metric indicates the proportion of variance in the output variable (the rating) that is predictable from the input variable (the review text). The best possible score is 1.0, which indicates the model always predicts the correct rating. The scikit-learn library provides several [evaluation metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) for machine learning models, including $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate the model with R^2'''\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true=test_reviews['Rating'], y_pred=test_reviews['MLP_BOW_Pred_Rating'])\n",
    "print(\"COEFFICIENT OF DETERMINATION (R2): {:3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full test dataset of 25,000 reviews, the $R^2$ for this model is 0.545692."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Alternative input to MLP: continuous bag-of-words vectors\n",
    "\n",
    "An alternative to the traditional bag-of-words representation is to encode sequences as a combination of their individual word embeddings. A word embedding is an n-dimensional vector of real values that together are intended to encode the \"meaning\" of a word. Word embedding models explicitly learn to represent words by trying to correctly predict other words that appear in the same context (or alternatively, trying to predict a word based on the context words). The result of these models are embedding vectors for words such that words with similar meanings (should) end up having similar vectors.\n",
    "\n",
    "####  spaCy word embeddings\n",
    "\n",
    "The spaCy library provides [GloVe embeddings](https://spacy.io/usage/vectors-similarity) for each word, which can be accessed simply with word.vector after loading the text into spaCy. There are 300 dimensions in these embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vector = encoder(\"creepy\").vector\n",
    "print(emb_vector)\n",
    "print(\"SHAPE:\", emb_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also has a built-in similarity function that returns the cosine similarity between the GloVe vectors for two words. For example, the vector for \"creepy\" is more similar to that of \"scary\" than \"nice\", as expected. See the link to the spaCy documentation for other functions that operate on the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder(\"creepy\").similarity(encoder(\"scary\")))\n",
    "print(encoder(\"creepy\").similarity(encoder(\"nice\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Combining embeddings\n",
    "\n",
    "We can use the embeddings as an alternative to the simple bag-of-words input to the model, by averaging the embeddings for all words in the review across each corresponding dimension (you could also sum them). So instead of having an input matrix with a column for each word in the lexicon, each column represents a word embedding dimension. This is referred to as a continuous bag-of-words vector. The advantage of this representation over the standard bag-of-words representation is that it more explicitly represents the meaning of the words in the review. For example, two reviews may express similar content (and have similar ratings) but may vary in the exact words they use, so their continous bag-of-word vectors may be more similar than their standard bag-of-words vectors. The model may more readily observe that these reviews should receive similar ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''First encode reviews as sequences of word embeddings'''\n",
    "\n",
    "def text_to_emb_seqs(seqs):\n",
    "    emb_seqs = [numpy.array([word.vector for word in encoder(seq)]) for seq in seqs]\n",
    "    return emb_seqs\n",
    "    \n",
    "emb_train_reviews = text_to_emb_seqs(train_reviews['Review'])\n",
    "\n",
    "#Example of word embedding sequence for first review\n",
    "pandas.DataFrame(list(zip(train_reviews['Tokenized_Review'][0], emb_train_reviews[0])),\n",
    "                columns=['Word', 'Embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Encode reviews as continuous bag-of-words (mean of word embeddings)'''\n",
    "\n",
    "def emb_seqs_to_cont_bows(emb_seqs):\n",
    "    cont_bow_seqs =  numpy.array([numpy.mean(emb_seq, axis=0) for emb_seq in emb_seqs])\n",
    "    return cont_bow_seqs\n",
    "\n",
    "cont_bow_train_reviews = emb_seqs_to_cont_bows(emb_train_reviews)\n",
    "\n",
    "print(\"TRAIN INPUT:\\n\", cont_bow_train_reviews)\n",
    "print(\"SHAPE:\", cont_bow_train_reviews.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Continuous bag-of-words MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the same MLP model to predict ratings from the reviews encoded as continuous bag-of-words vectors. The only difference between the parameters of this model compared to the previous model is that n_input_nodes is equal to the number of embedding dimensions instead of the number of words in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_cont_bow_model = create_mlp_model(n_input_nodes=cont_bow_train_reviews.shape[-1], n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "mlp_cont_bow_model.fit(x=cont_bow_train_reviews, y=train_reviews['Rating'], batch_size=20, epochs=5)\n",
    "mlp_cont_bow_model.save('example_model/mlp_cont_bow/model.h5') #save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I'll load this same model I previously trained on all 25,000 reviews in the training set and apply it to the example test set of 100 reviews.\n",
    "\n",
    "### Exercise \n",
    "\n",
    "Load the pretrained model ! \n",
    "Path :``'pretrained_model/mlp_cont_bow/model.h5'``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load saved model'''\n",
    "### ENTER YOUR CODE HERE ( 1 line )\n",
    "mlp_cont_bow_model = ....\n",
    "### END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transform test reviews to a continuous bag-of-words matrix'''\n",
    "\n",
    "cont_bow_test_reviews = emb_seqs_to_cont_bows(text_to_emb_seqs(test_reviews['Review']))\n",
    "\n",
    "print(\"TEST INPUT:\\n\", cont_bow_test_reviews)\n",
    "print(\"SHAPE:\", cont_bow_test_reviews.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show ratings predicted by this model alongside previous model and actual ratings'''\n",
    "\n",
    "#Since ratings are integers, need to round predicted rating to nearest integer\n",
    "test_reviews['MLP_Cont_BOW_Pred_Rating'] = numpy.round(mlp_cont_bow_model.predict(cont_bow_test_reviews)[:,0]).astype(int)\n",
    "test_reviews[['Review', 'Rating', 'MLP_BOW_Pred_Rating', 'MLP_Cont_BOW_Pred_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate the model with R^2'''\n",
    "\n",
    "r2 = r2_score(y_true=test_reviews['Rating'], y_pred=test_reviews['MLP_Cont_BOW_Pred_Rating'])\n",
    "print(\"COEFFICIENT OF DETERMINATION (R2): {:3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full test dataset of 25,000 reviews, the $R^2$ for this model is 0.494190. So it turns out this model overall does not actually do better at predicting ratings than the standard bag-of-words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Recurrent Neural Network \n",
    "\n",
    "Now I'll show how this same task can be modeled with an RNN, which processes text sequentially.\n",
    "\n",
    "###  Numerical lists to matrices\n",
    "\n",
    "The input representation for the RNN is different from the MLP because it explicitly encodes the order of words in the review. We'll return to the lists of the word indices contained in train_reviews['Review_Idxs']. The input to the model will be these number sequences themselves. We need to put all the reviews in the training set into a single matrix, where each row is a review and each column is a word index in that sequence. This enables the model to process multiple sequences in parallel (batches) as opposed to one at a time. Using batches significantly speeds up training. However, each review has a different number of words, so we create a padded matrix equal to the length on the longest review in the training set. For all reviews with fewer words, we prepend the row with zeros representing an empty word position. This is why the number 0 was not assigned as a word index in the lexicon. We can tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Create ``pad_idx_seqs`` function. This function return a `pad_sequence(idx_seqs)`. Remember, you have to get the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create a padded matrix of input reviews'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "### ENTER YOUR CODE HERE (3 lines)\n",
    "\n",
    "\n",
    "\n",
    "### END \n",
    "train_padded_idxs = pad_idx_seqs(train_reviews['Review_Idxs'])\n",
    "\n",
    "print(\"TRAIN INPUT:\\n\", train_padded_idxs)\n",
    "print(\"SHAPE:\", train_padded_idxs.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Layers\n",
    "\n",
    "We'll use the same scheme as before (the Functional API) to assemble the RNN. The RNN will have four layers:\n",
    "\n",
    "**1. Input**: The input layer takes in the matrix of word indices.\n",
    "\n",
    "**2. Embedding**: A [layer](https://keras.io/layers/embeddings/) that converts integer word indices into distributed vector representations (embeddings), which were introduced above. The difference here is that rather than plugging in embeddings from a pretrained model as before, the word embeddings will be learned inside the model itself. Thus, the input to the model will be the word indices rather than their embeddings, and the embedding values will change as the model is trained. The mask_zero=True parameter in this layer indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**3. GRU**: A [recurrent (GRU) hidden layer](https://keras.io/layers/recurrent/), the central component of the model. As it observes each word in the review, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the review at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer (see the materials at the bottom for an explanation of the difference). This layer outputs the last hidden state of the sequence (i.e. the hidden representation of the review after its last word is observed).\n",
    "\n",
    "**4. Dense**: An output [layer](https://keras.io/layers/core/#dense) that predicts the rating for the review based on its GRU representation given by the previous layer. This is the same output layer used in the MLP, so it has one dimension that contains a continuous value (the rating).\n",
    "\n",
    "###  Parameters\n",
    "\n",
    "Our function for creating the RNN takes the following parameters:\n",
    "\n",
    "**n_input_nodes**: As with the standard bag-of-words MLP, this is the number of unique words in the lexicon, plus one to account for the padding represented by 0 values. This indicates the number of rows in the embedding layer, where each row corresponds to a word.\n",
    "\n",
    "**n_embedding_nodes**: the number of dimensions (units) in the embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the GRU hidden layer. Like the embedding layer, this can be freely chosen. Here, it is set to 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Build your model. Create a `create_rnn_model` function. \n",
    "1. Input: The input layer takes in the matrix of word indices.\n",
    "2. Embedding: A layer that converts integer word indices into distributed vector representations (embeddings), which were introduced above.\n",
    "3. GRU: A recurrent (GRU) hidden layer, the central component of the model\n",
    "4. Dense : An output layer that predicts the rating for the review based on its GRU representation given by the previous layer. \n",
    "5. Compile your model. loss = \"mean_squared_error\" and \"adam\" optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_rnn_model(n_input_nodes, n_embedding_nodes, n_hidden_nodes):\n",
    "    #### ENTER YOUR CODE HERE \n",
    "    # Layer 1 -  Technically the shape of this layer is (batch_size, len(train_padded_idxs)).\n",
    "    # However, both the batch size and the length of the input matrix can be inferred from the input at training time. \n",
    "    # The batch size is implicitly included in the shape of the input, so it does not need to \n",
    "    # be specified as a dimension of the input. None can be given as placeholder for the input matrix length.\n",
    "    # By defining it as None, the model is flexible in accepting inputs with different lengths.\n",
    "\n",
    "    \n",
    "    # Layer 2 : Embedding\n",
    "    #mask_zero tells the model to ignore 0 values (padding)\n",
    "    \n",
    "    \n",
    "    #Output shape = (batch_size, input_matrix_length, n_embedding_nodes)\n",
    "    \n",
    "    # Layer 3 : GRU \n",
    "    \n",
    "    #Output shape = (batch_size, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 4 : Dense\n",
    "\n",
    "    #Output shape = (batch_size, 1)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "\n",
    "    \n",
    "    ### END\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = create_rnn_model(n_input_nodes=len(lexicon) + 1, n_embedding_nodes=300, n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training\n",
    "\n",
    "The training function is exactly the same for the RNN as above, just with the padded review matrix now provided as the input.\n",
    "\n",
    "#### Exercise \n",
    "\n",
    "Fit and save your model and save also your lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train the model'''\n",
    "### ENTER YOUR CODE HERE\n",
    "# Fit your model\n",
    "\n",
    "# Save model \n",
    "\n",
    "# Save lexicon to new model folder - same lexicon as above\n",
    "\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prediction\n",
    "\n",
    "#### Exercise \n",
    "\n",
    "Load your lexicon ans load your model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load saved model'''\n",
    "### ENTER YOUR CODE HERE \n",
    "# Load lexicon (2 lines)\n",
    "\n",
    "\n",
    "# Load RNN model (2 lines)\n",
    "\n",
    "\n",
    "### END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Put test reviews in padded matrix'''\n",
    "\n",
    "test_reviews['Review_Idxs'] = tokens_to_idxs(token_seqs=test_reviews['Tokenized_Review'],\n",
    "                                             lexicon=rnn_lexicon)\n",
    "test_padded_idxs = pad_idx_seqs(test_reviews['Review_Idxs'])\n",
    "\n",
    "print(\"TEST INPUT:\\n\", test_padded_idxs)\n",
    "print(\"SHAPE:\", test_padded_idxs.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show ratings predicted by RNN alongside the other models' ratings'''\n",
    "\n",
    "#Since ratings are integers, need to round predicted rating to nearest integer\n",
    "test_reviews['RNN_Pred_Rating'] = numpy.round(rnn_model.predict(test_padded_idxs)[:,0]).astype(int)\n",
    "test_reviews[['Review', 'Rating', 'MLP_BOW_Pred_Rating', 'MLP_Cont_BOW_Pred_Rating', 'RNN_Pred_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate the model with R^2'''\n",
    "\n",
    "r2 = r2_score(y_true=test_reviews['Rating'], y_pred=test_reviews['RNN_Pred_Rating'])\n",
    "print(\"COEFFICIENT OF DETERMINATION (R2): {:3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the full test dataset of 25,000 reviews, the $R^2$ for this model is 0.622525. So the RNN outperforms the continuous bag-of-words MLP as well as the standard bag-of-words approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing data inside the model\n",
    "\n",
    "To help visualize the data representation inside the model, we can look at the output of each layer in a model individually. Keras' Functional API lets you derive a new model with the layers from an existing model, so you can define the output to be a layer below the output layer in the original model. Calling predict() on this new model will produce the output of that layer for a given input. Of course, glancing at the numbers by themselves doesn't provide any interpretation of what the model has learned (although there are opportunities to [interpret these values](https://www.civisanalytics.com/blog/interpreting-visualizing-neural-networks-text-processing/)), but seeing them verifies the model is just a series of transformations from one matrix to another. The model stores its layers as the list model.layers, and you can retrieve specific layer by its position index in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show the output of the RNN embedding layer (second layer) for the test reviews'''\n",
    "\n",
    "embedding_layer = Model(inputs=rnn_model.layers[0].input, \n",
    "                        outputs=rnn_model.layers[1].output) #embedding layer is 2nd layer (index 1)\n",
    "embedding_output = embedding_layer.predict(test_padded_idxs)\n",
    "print(\"EMBEDDING LAYER OUTPUT SHAPE:\", embedding_output.shape)\n",
    "print(embedding_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also easy to look at the weight matrices that connect the layers. The get_weights() function will show the incoming weights for a particular layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show weights that connect the RNN hidden layer to the output layer (final layer)'''\n",
    "\n",
    "hidden_to_output_weights = rnn_model.layers[-1].get_weights()[0]\n",
    "print(\"HIDDEN-TO_OUTPUT WEIGHTS SHAPE:\", hidden_to_output_weights.shape)\n",
    "print(hidden_to_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As mentioned above, the models shown here could be applied to any task where the goal is to predict a score for a particular sequence. For ratings prediction, this output is ordinal, but it could also be categorical with a few simple changes to the output layer of the model. My other notebooks in this repository for language modeling/generation and part-of-speech tagging demonstrate this type of prediction with categorical variables. They also show how to build an RNN in Keras when the output is a sequence of labels, rather than a single value as shown here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
